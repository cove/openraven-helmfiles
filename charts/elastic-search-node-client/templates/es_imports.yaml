apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "elastic-search-node-client.fullname" . }}-imports
  labels:
{{ include "elastic-search-node-client.labels" . | indent 4 }}
spec:
  backoffLimit: 4
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "elastic-search-node-client.name" . }}-imports
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
    {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
    {{- end }}
      serviceAccountName: {{ template "elastic-search-node-client.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      restartPolicy: OnFailure
      {{ $kibanaUrl := .Values.kibana_url }}
      {{ with .Values.elasticsearch_load }}
      containers:
      - name: elasticdump-imports
        image: {{ .image }}
        imagePullPolicy: {{ $.Values.image.pullPolicy }}
        command:
          - /bin/sh
          - -exc
          # language=sh
          - |
            # yes, even 10 minutes may not be enough for ES to be a-ok
            # and realistically we want this to wait forever, but we'll start here
            for _ in $(seq 1 60); do
              if wget --quiet -O /dev/null -T 1 {{ .url }}/_cat/health >/dev/null 2>&1; then
                break
              fi
              sleep 10
            done
            wget https://{{ .bucketname }}.s3.amazonaws.com/{{ .keyprefix }}/assetgrouptemplate.json
            wget https://{{ .bucketname }}.s3.amazonaws.com/{{ .keyprefix }}/assetgroups.json
            wget https://{{ .bucketname }}.s3.amazonaws.com/{{ .keyprefix }}/configtemplate.json
            elasticdump \
              --input=./assetgrouptemplate.json \
              --output={{ .url }}/assetgroup \
              --type=template
            elasticdump \
              --input=./assetgroups.json \
              --output={{ .url }}/assetgroup \
              --type=data
            elasticdump \
              --input=./configtemplate.json \
              --output={{ .url }}/configservice \
              --type=template

      - name: kibana-and-es-updates
        imagePullPolicy: {{ $.Values.image.pullPolicy }}
        image: docker.io/library/python:3.8
        command:
          - /bin/bash
          - -exc
          # language=sh
          - |
            # Explicitly not throwing a -f to curl. The indexes wont always be there. Think of this block as a way to
            # apply settings to existing clusters.
            curl -sSL -XPUT -H "Content-Type: application/json" \
                -d '{"index.max_docvalue_fields_search": 250}' \
                {{ .url }}/_all/_settings
            curl -sSL -XPUT -H "Content-Type: application/json" \
                -d '{"index.number_of_replicas": 2}' \
                {{ .url }}/aws*/_settings
            curl -sSL -XPUT -H "Content-Type: application/json" \
                -d '{"index.number_of_replicas": 2}' \
            {{ .url }}/assetgroup/_settings
            curl -sSL -XPUT -H "Content-Type: application/json" \
                -d '{"index.number_of_replicas": 2}' \
            {{ .url }}/orvn*/_settings

            out_fn=dashboards.ndjson
            curl -fsSLo ${out_fn} https://{{ .bucketname }}.s3.amazonaws.com/{{ .keyprefix }}/dashboards.ndjson
            for _ in 1 2 3 4 5 6 7 8 9 10; do
              if curl -fsSL -H "kbn-xsrf: true" \
                     --form "file=@${out_fn}" \
                    "{{ $kibanaUrl }}/api/saved_objects/_import?overwrite=true"
              then
                break
              fi
              sleep 30 # 10 seconds wasnt enough
            done
      {{ end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
    {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
    {{- end }}
    {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
    {{- end }}
