
       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 30, Failed checks: 34, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_15: "Image Pull Policy should be Always"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-account-management
		7  |   labels:
		8  |     app.kubernetes.io/name: account-management
		9  |     helm.sh/chart: account-management-0.257894263.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: account-management
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-account-management
		29  |   labels:
		30  |     app.kubernetes.io/name: account-management
		31  |     helm.sh/chart: account-management-0.257894263.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: account-management
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: account-management
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/account-management.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/account-management.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/account-management.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "account-management",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-account-management
		29  |   labels:
		30  |     app.kubernetes.io/name: account-management
		31  |     helm.sh/chart: account-management-0.257894263.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: account-management
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: account-management
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/account-management.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/account-management.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/account-management.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "account-management",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-account-management
		29  |   labels:
		30  |     app.kubernetes.io/name: account-management
		31  |     helm.sh/chart: account-management-0.257894263.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: account-management
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: account-management
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/account-management.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/account-management.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/account-management.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "account-management",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-account-management
		29  |   labels:
		30  |     app.kubernetes.io/name: account-management
		31  |     helm.sh/chart: account-management-0.257894263.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: account-management
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: account-management
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/account-management.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/account-management.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/account-management.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "account-management",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:25-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-account-management
		29  |   labels:
		30  |     app.kubernetes.io/name: account-management
		31  |     helm.sh/chart: account-management-0.257894263.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: account-management
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: account-management
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/account-management.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/account-management.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/account-management.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "account-management",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-account-management.default
	File: ./charts/account-management.yaml:127-174
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		127 | apiVersion: networking.k8s.io/v1beta1
		128 | kind: Ingress
		129 | metadata:
		130 |   name: RELEASE-NAME-account-management
		131 |   labels:
		132 |     app.kubernetes.io/name: account-management
		133 |     helm.sh/chart: account-management-0.257894263.0
		134 |     app.kubernetes.io/instance: RELEASE-NAME
		135 |     app.kubernetes.io/version: "1.0"
		136 |     app.kubernetes.io/managed-by: Helm
		137 |   annotations:
		138 |     nginx.ingress.kubernetes.io/auth-response-headers: x-auth-request-user, x-auth-request-email, x-auth-request-access-token
		139 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		140 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		141 |       proxy_set_header cookie "";
		142 |       auth_request_set $auth_cookie $upstream_http_set_cookie;
		143 |       add_header Set-Cookie $auth_cookie;
		144 |       auth_request_set $bearer_token $upstream_http_x_auth_request_access_token;
		145 |       proxy_set_header authorization 'Bearer $bearer_token';
		146 | spec:
		147 |   rules:
		148 |     - host: 
		149 |       http:
		150 |         paths:
		151 |           - path: /api/user
		152 |             backend:
		153 |               serviceName: RELEASE-NAME-account-management
		154 |               servicePort: 80
		155 |           - path: /api/orgs
		156 |             backend:
		157 |               serviceName: RELEASE-NAME-account-management
		158 |               servicePort: 80
		159 |           - path: /api/server
		160 |             backend:
		161 |               serviceName: RELEASE-NAME-account-management
		162 |               servicePort: 80
		163 |           - path: /api/scan
		164 |             backend:
		165 |               serviceName: RELEASE-NAME-account-management
		166 |               servicePort: 80
		167 |           - path: /api/cloudwatcher
		168 |             backend:
		169 |               serviceName: RELEASE-NAME-account-management
		170 |               servicePort: 80
		171 |           - path: /api/integrations
		172 |             backend:
		173 |               serviceName: RELEASE-NAME-account-management
		174 |               servicePort: 80


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		177 | apiVersion: v1
		178 | kind: Pod
		179 | metadata:
		180 |   name: "RELEASE-NAME-account-management-test-connection"
		181 |   labels:
		182 |     app.kubernetes.io/name: account-management
		183 |     helm.sh/chart: account-management-0.257894263.0
		184 |     app.kubernetes.io/instance: RELEASE-NAME
		185 |     app.kubernetes.io/version: "1.0"
		186 |     app.kubernetes.io/managed-by: Helm
		187 |   annotations:
		188 |     "helm.sh/hook": test-success
		189 | spec:
		190 |   containers:
		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']
		195 |   restartPolicy: Never


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		177 | apiVersion: v1
		178 | kind: Pod
		179 | metadata:
		180 |   name: "RELEASE-NAME-account-management-test-connection"
		181 |   labels:
		182 |     app.kubernetes.io/name: account-management
		183 |     helm.sh/chart: account-management-0.257894263.0
		184 |     app.kubernetes.io/instance: RELEASE-NAME
		185 |     app.kubernetes.io/version: "1.0"
		186 |     app.kubernetes.io/managed-by: Helm
		187 |   annotations:
		188 |     "helm.sh/hook": test-success
		189 | spec:
		190 |   containers:
		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']
		195 |   restartPolicy: Never


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		177 | apiVersion: v1
		178 | kind: Pod
		179 | metadata:
		180 |   name: "RELEASE-NAME-account-management-test-connection"
		181 |   labels:
		182 |     app.kubernetes.io/name: account-management
		183 |     helm.sh/chart: account-management-0.257894263.0
		184 |     app.kubernetes.io/instance: RELEASE-NAME
		185 |     app.kubernetes.io/version: "1.0"
		186 |     app.kubernetes.io/managed-by: Helm
		187 |   annotations:
		188 |     "helm.sh/hook": test-success
		189 | spec:
		190 |   containers:
		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']
		195 |   restartPolicy: Never


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		177 | apiVersion: v1
		178 | kind: Pod
		179 | metadata:
		180 |   name: "RELEASE-NAME-account-management-test-connection"
		181 |   labels:
		182 |     app.kubernetes.io/name: account-management
		183 |     helm.sh/chart: account-management-0.257894263.0
		184 |     app.kubernetes.io/instance: RELEASE-NAME
		185 |     app.kubernetes.io/version: "1.0"
		186 |     app.kubernetes.io/managed-by: Helm
		187 |   annotations:
		188 |     "helm.sh/hook": test-success
		189 | spec:
		190 |   containers:
		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']
		195 |   restartPolicy: Never


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		177 | apiVersion: v1
		178 | kind: Pod
		179 | metadata:
		180 |   name: "RELEASE-NAME-account-management-test-connection"
		181 |   labels:
		182 |     app.kubernetes.io/name: account-management
		183 |     helm.sh/chart: account-management-0.257894263.0
		184 |     app.kubernetes.io/instance: RELEASE-NAME
		185 |     app.kubernetes.io/version: "1.0"
		186 |     app.kubernetes.io/managed-by: Helm
		187 |   annotations:
		188 |     "helm.sh/hook": test-success
		189 | spec:
		190 |   containers:
		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']
		195 |   restartPolicy: Never


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default
	File: ./charts/account-management.yaml:177-195
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		177 | apiVersion: v1
		178 | kind: Pod
		179 | metadata:
		180 |   name: "RELEASE-NAME-account-management-test-connection"
		181 |   labels:
		182 |     app.kubernetes.io/name: account-management
		183 |     helm.sh/chart: account-management-0.257894263.0
		184 |     app.kubernetes.io/instance: RELEASE-NAME
		185 |     app.kubernetes.io/version: "1.0"
		186 |     app.kubernetes.io/managed-by: Helm
		187 |   annotations:
		188 |     "helm.sh/hook": test-success
		189 | spec:
		190 |   containers:
		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']
		195 |   restartPolicy: Never


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-account-management.default (container 0) - account-management
	File: ./charts/account-management.yaml:64-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		64  |         - name: account-management
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/account-management:257894263"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             initialDelaySeconds: 15
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           readinessProbe:
		79  |             initialDelaySeconds: 15
		80  |             httpGet:
		81  |               path: /actuator/health
		82  |               port: 8080
		83  |           resources:
		84  |             limits:
		85  |               memory: 512Mi
		86  |             requests:
		87  |               memory: 512Mi
		88  |           env:
		89  |             - name: SPRING_PROFILES_ACTIVE
		90  |               value: default, prod
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: SERVER_PORT
		94  |               value: "80"
		95  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		96  |               value: "128KB"
		97  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		98  |               value: 
		99  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		100 |               value: 
		101 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-ID
		102 |               value: 
		103 |             - name: OPENRAVEN_APP_V1_CLUSTER_ADMIN-CLIENT-SECRET
		104 |               value: 
		105 |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		106 |               value: 
		107 |             - name: SENTRY_DSN
		108 |               value: "https://fc5ad9c6cb24415198653b291c7f7e10@o322024.ingest.sentry.io/5379176"
		109 |             - name: SENTRY_ENVIRONMENT
		110 |               value: 
		111 |             - name: SENTRY_RELEASE
		112 |               value: "0.257894263.0"
		113 |             - name: SENTRY_EXTRA
		114 |               value: "groupId:"
		115 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		116 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		117 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		118 |               value: 
		119 |             - name: JAVA_TOOL_OPTIONS
		120 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		121 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-CHANNEL
		122 |               value: 
		123 |             - name: OPENRAVEN_APP_V1_CLUSTER_RELEASE-VERSION
		124 |               value:


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Pod.RELEASE-NAME-account-management-test-connection.default (container 0) - wget
	File: ./charts/account-management.yaml:191-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		191 |     - name: wget
		192 |       image: busybox
		193 |       command: ['wget']
		194 |       args:  ['RELEASE-NAME-account-management:80']



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 18, Failed checks: 15, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-asset-groups
		7  |   labels:
		8  |     app.kubernetes.io/name: asset-groups
		9  |     helm.sh/chart: asset-groups-0.268013712.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: asset-groups
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-asset-groups
		29  |   labels:
		30  |     app.kubernetes.io/name: asset-groups
		31  |     helm.sh/chart: asset-groups-0.268013712.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: asset-groups
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: asset-groups
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/asset-groups.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/asset-groups.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/asset-groups.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "asset-groups",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-asset-groups
		29  |   labels:
		30  |     app.kubernetes.io/name: asset-groups
		31  |     helm.sh/chart: asset-groups-0.268013712.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: asset-groups
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: asset-groups
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/asset-groups.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/asset-groups.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/asset-groups.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "asset-groups",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-asset-groups
		29  |   labels:
		30  |     app.kubernetes.io/name: asset-groups
		31  |     helm.sh/chart: asset-groups-0.268013712.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: asset-groups
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: asset-groups
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/asset-groups.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/asset-groups.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/asset-groups.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "asset-groups",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-asset-groups
		29  |   labels:
		30  |     app.kubernetes.io/name: asset-groups
		31  |     helm.sh/chart: asset-groups-0.268013712.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: asset-groups
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: asset-groups
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/asset-groups.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/asset-groups.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/asset-groups.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "asset-groups",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-asset-groups
		29  |   labels:
		30  |     app.kubernetes.io/name: asset-groups
		31  |     helm.sh/chart: asset-groups-0.268013712.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: asset-groups
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: asset-groups
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/asset-groups.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/asset-groups.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/asset-groups.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "asset-groups",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-asset-groups.default
	File: ./charts/asset-groups.yaml:105-129
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		105 | apiVersion: networking.k8s.io/v1beta1
		106 | kind: Ingress
		107 | metadata:
		108 |   name: RELEASE-NAME-asset-groups
		109 |   labels:
		110 |     app.kubernetes.io/name: asset-groups
		111 |     helm.sh/chart: asset-groups-0.268013712.0
		112 |     app.kubernetes.io/instance: RELEASE-NAME
		113 |     app.kubernetes.io/version: "1.0"
		114 |     app.kubernetes.io/managed-by: Helm
		115 |   annotations:
		116 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		117 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		118 |       proxy_set_header cookie "";
		119 |       auth_request_set $auth_cookie $upstream_http_set_cookie;
		120 |       add_header Set-Cookie $auth_cookie;
		121 | spec:
		122 |   rules:
		123 |     - host: 
		124 |       http:
		125 |         paths:
		126 |           - path: /api/asset-groups
		127 |             backend:
		128 |               serviceName: RELEASE-NAME-asset-groups
		129 |               servicePort: 80


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-asset-groups.default (container 0) - asset-groups
	File: ./charts/asset-groups.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		64  |         - name: asset-groups
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/asset-groups-repo-docker:268013712"
		68  |           imagePullPolicy: IfNotPresent
		69  |           ports:
		70  |             - name: http
		71  |               containerPort: 80
		72  |               protocol: TCP
		73  |           livenessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           readinessProbe:
		78  |             httpGet:
		79  |               path: /actuator/health
		80  |               port: 8080
		81  |           resources:
		82  |             limits:
		83  |               memory: 1Gi
		84  |             requests:
		85  |               memory: 1Gi
		86  |           env:
		87  |             - name: SPRING_PROFILES_ACTIVE
		88  |               value: default,prod
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: MANAGEMENT_SERVER_PORT
		92  |               value: "8080"
		93  |             - name: JAVA_TOOL_OPTIONS
		94  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		95  |             - name: SENTRY_DSN
		96  |               value: "https://08c4669ca41b472e8a7e5a1398056d3f@o322024.ingest.sentry.io/5433951"
		97  |             - name: SENTRY_ENVIRONMENT
		98  |               value: 
		99  |             - name: SENTRY_RELEASE
		100 |               value: "0.268013712.0"
		101 |             - name: SENTRY_EXTRA
		102 |               value: "groupId:"



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 


       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 30, Failed checks: 33, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_15: "Image Pull Policy should be Always"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-cloudwatcher
		7  |   labels:
		8  |     helm.sh/chart: cloudwatcher-0.177202707.0
		9  |     app.kubernetes.io/name: cloudwatcher
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "0.0.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: cloudwatcher
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cloudwatcher
		29  |   labels:
		30  |     helm.sh/chart: cloudwatcher-0.177202707.0
		31  |     app.kubernetes.io/name: cloudwatcher
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cloudwatcher
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cloudwatcher
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cloudwatcher.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cloudwatcher.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cloudwatcher.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "cloudwatcher",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cloudwatcher
		29  |   labels:
		30  |     helm.sh/chart: cloudwatcher-0.177202707.0
		31  |     app.kubernetes.io/name: cloudwatcher
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cloudwatcher
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cloudwatcher
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cloudwatcher.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cloudwatcher.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cloudwatcher.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "cloudwatcher",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cloudwatcher
		29  |   labels:
		30  |     helm.sh/chart: cloudwatcher-0.177202707.0
		31  |     app.kubernetes.io/name: cloudwatcher
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cloudwatcher
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cloudwatcher
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cloudwatcher.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cloudwatcher.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cloudwatcher.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "cloudwatcher",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cloudwatcher
		29  |   labels:
		30  |     helm.sh/chart: cloudwatcher-0.177202707.0
		31  |     app.kubernetes.io/name: cloudwatcher
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cloudwatcher
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cloudwatcher
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cloudwatcher.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cloudwatcher.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cloudwatcher.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "cloudwatcher",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default
	File: ./charts/cloudwatcher.yaml:25-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cloudwatcher
		29  |   labels:
		30  |     helm.sh/chart: cloudwatcher-0.177202707.0
		31  |     app.kubernetes.io/name: cloudwatcher
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cloudwatcher
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cloudwatcher
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cloudwatcher.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cloudwatcher.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cloudwatcher.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "cloudwatcher",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       serviceAccountName: default
		61  |       securityContext:
		62  |         {}
		63  |       containers:
		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		105 | apiVersion: v1
		106 | kind: Pod
		107 | metadata:
		108 |   name: "RELEASE-NAME-cloudwatcher-test-connection"
		109 |   labels:
		110 | 
		111 |     helm.sh/chart: cloudwatcher-0.177202707.0
		112 |     app.kubernetes.io/name: cloudwatcher
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     "helm.sh/hook": test-success
		118 | spec:
		119 |   containers:
		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']
		124 |   restartPolicy: Never


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		105 | apiVersion: v1
		106 | kind: Pod
		107 | metadata:
		108 |   name: "RELEASE-NAME-cloudwatcher-test-connection"
		109 |   labels:
		110 | 
		111 |     helm.sh/chart: cloudwatcher-0.177202707.0
		112 |     app.kubernetes.io/name: cloudwatcher
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     "helm.sh/hook": test-success
		118 | spec:
		119 |   containers:
		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']
		124 |   restartPolicy: Never


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		105 | apiVersion: v1
		106 | kind: Pod
		107 | metadata:
		108 |   name: "RELEASE-NAME-cloudwatcher-test-connection"
		109 |   labels:
		110 | 
		111 |     helm.sh/chart: cloudwatcher-0.177202707.0
		112 |     app.kubernetes.io/name: cloudwatcher
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     "helm.sh/hook": test-success
		118 | spec:
		119 |   containers:
		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']
		124 |   restartPolicy: Never


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		105 | apiVersion: v1
		106 | kind: Pod
		107 | metadata:
		108 |   name: "RELEASE-NAME-cloudwatcher-test-connection"
		109 |   labels:
		110 | 
		111 |     helm.sh/chart: cloudwatcher-0.177202707.0
		112 |     app.kubernetes.io/name: cloudwatcher
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     "helm.sh/hook": test-success
		118 | spec:
		119 |   containers:
		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']
		124 |   restartPolicy: Never


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		105 | apiVersion: v1
		106 | kind: Pod
		107 | metadata:
		108 |   name: "RELEASE-NAME-cloudwatcher-test-connection"
		109 |   labels:
		110 | 
		111 |     helm.sh/chart: cloudwatcher-0.177202707.0
		112 |     app.kubernetes.io/name: cloudwatcher
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     "helm.sh/hook": test-success
		118 | spec:
		119 |   containers:
		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']
		124 |   restartPolicy: Never


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default
	File: ./charts/cloudwatcher.yaml:105-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		105 | apiVersion: v1
		106 | kind: Pod
		107 | metadata:
		108 |   name: "RELEASE-NAME-cloudwatcher-test-connection"
		109 |   labels:
		110 | 
		111 |     helm.sh/chart: cloudwatcher-0.177202707.0
		112 |     app.kubernetes.io/name: cloudwatcher
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     "helm.sh/hook": test-success
		118 | spec:
		119 |   containers:
		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']
		124 |   restartPolicy: Never


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-cloudwatcher.default (container 0) - cloudwatcher
	File: ./charts/cloudwatcher.yaml:64-102
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		64  |         - name: cloudwatcher
		65  |           securityContext:
		66  |             {}
		67  |           image: "registry.gitlab.com/openraven/open/cloudwatcher-repo-docker:177202707"
		68  |           imagePullPolicy: IfNotPresent
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |             initialDelaySeconds: 15
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |             initialDelaySeconds: 15
		79  |           resources:
		80  |             limits:
		81  |               memory: 1G
		82  |             requests:
		83  |               memory: 1G
		84  |           env:
		85  |             - name: SPRING_PROFILES_ACTIVE
		86  |               value: default, prod, extractor
		87  |             - name: MANAGEMENT_SERVER_PORT
		88  |               value: "8080"
		89  |             - name: SERVER_PORT
		90  |               value: "80"
		91  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		92  |               value: "128KB"
		93  |             - name: SENTRY_DSN
		94  |               value: "https://de53a7acdf9d459184af52cd52af4521@o322024.ingest.sentry.io/2084588"
		95  |             - name: SENTRY_EXTRA
		96  |               value: "groupId:"
		97  |             - name: SENTRY_RELEASE
		98  |               value: "0.177202707.0"
		99  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		100 |               value: 
		101 |             - name: JAVA_TOOL_OPTIONS
		102 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Pod.RELEASE-NAME-cloudwatcher-test-connection.default (container 0) - wget
	File: ./charts/cloudwatcher.yaml:120-123
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		120 |     - name: wget
		121 |       image: busybox
		122 |       command: ['wget']
		123 |       args:  ['RELEASE-NAME-cloudwatcher:80']



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 34, Failed checks: 31, Skipped checks: 0

Check: CKV_K8S_41: "Ensure that default service accounts are not actively used"
	PASSED for resource: ServiceAccount.cluster-upgrade-sa.default
	File: ./charts/cluster-upgrade.yaml:3-12
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_38

Check: CKV_K8S_42: "Ensure that default service accounts are not actively used"
	PASSED for resource: ClusterRoleBinding.RELEASE-NAME-cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:15-32
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_38

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: ServiceAccount.cluster-upgrade-sa.default
	File: ./charts/cluster-upgrade.yaml:3-12
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: ServiceAccount
		5  | metadata:
		6  |   name: cluster-upgrade-sa
		7  |   labels:
		8  |     app.kubernetes.io/name: cluster-upgrade
		9  |     helm.sh/chart: cluster-upgrade-0.209505439.4
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		35  | apiVersion: apps/v1
		36  | kind: Deployment
		37  | metadata:
		38  |   name: RELEASE-NAME-cluster-upgrade
		39  |   labels:
		40  |     app.kubernetes.io/name: cluster-upgrade
		41  |     helm.sh/chart: cluster-upgrade-0.209505439.4
		42  |     app.kubernetes.io/instance: RELEASE-NAME
		43  |     app.kubernetes.io/version: "1.0"
		44  |     app.kubernetes.io/managed-by: Helm
		45  | spec:
		46  |   replicas: 1
		47  |   selector:
		48  |     matchLabels:
		49  |       app.kubernetes.io/name: cluster-upgrade
		50  |       app.kubernetes.io/instance: RELEASE-NAME
		51  |   template:
		52  |     metadata:
		53  |       labels:
		54  |         app.kubernetes.io/name: cluster-upgrade
		55  |         app.kubernetes.io/instance: RELEASE-NAME
		56  |       annotations:
		57  |         iam.amazonaws.com/role: "orvn--cluster-upgrade"
		58  |         ad.datadoghq.com/cluster-upgrade.check_names: |
		59  |           ["openmetrics"]
		60  |         ad.datadoghq.com/cluster-upgrade.init_configs: |
		61  |           [{}]
		62  |         ad.datadoghq.com/cluster-upgrade.instances: |
		63  |           [
		64  |             {
		65  |               "prometheus_url": "http://%%host%%:9404/metrics",
		66  |               "namespace": "cluster-upgrade",
		67  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		68  |             }
		69  |           ]
		70  |     spec:
		71  |       serviceAccountName: cluster-upgrade-sa
		72  |       securityContext:
		73  |         {}
		74  |       containers:
		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		35  | apiVersion: apps/v1
		36  | kind: Deployment
		37  | metadata:
		38  |   name: RELEASE-NAME-cluster-upgrade
		39  |   labels:
		40  |     app.kubernetes.io/name: cluster-upgrade
		41  |     helm.sh/chart: cluster-upgrade-0.209505439.4
		42  |     app.kubernetes.io/instance: RELEASE-NAME
		43  |     app.kubernetes.io/version: "1.0"
		44  |     app.kubernetes.io/managed-by: Helm
		45  | spec:
		46  |   replicas: 1
		47  |   selector:
		48  |     matchLabels:
		49  |       app.kubernetes.io/name: cluster-upgrade
		50  |       app.kubernetes.io/instance: RELEASE-NAME
		51  |   template:
		52  |     metadata:
		53  |       labels:
		54  |         app.kubernetes.io/name: cluster-upgrade
		55  |         app.kubernetes.io/instance: RELEASE-NAME
		56  |       annotations:
		57  |         iam.amazonaws.com/role: "orvn--cluster-upgrade"
		58  |         ad.datadoghq.com/cluster-upgrade.check_names: |
		59  |           ["openmetrics"]
		60  |         ad.datadoghq.com/cluster-upgrade.init_configs: |
		61  |           [{}]
		62  |         ad.datadoghq.com/cluster-upgrade.instances: |
		63  |           [
		64  |             {
		65  |               "prometheus_url": "http://%%host%%:9404/metrics",
		66  |               "namespace": "cluster-upgrade",
		67  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		68  |             }
		69  |           ]
		70  |     spec:
		71  |       serviceAccountName: cluster-upgrade-sa
		72  |       securityContext:
		73  |         {}
		74  |       containers:
		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		35  | apiVersion: apps/v1
		36  | kind: Deployment
		37  | metadata:
		38  |   name: RELEASE-NAME-cluster-upgrade
		39  |   labels:
		40  |     app.kubernetes.io/name: cluster-upgrade
		41  |     helm.sh/chart: cluster-upgrade-0.209505439.4
		42  |     app.kubernetes.io/instance: RELEASE-NAME
		43  |     app.kubernetes.io/version: "1.0"
		44  |     app.kubernetes.io/managed-by: Helm
		45  | spec:
		46  |   replicas: 1
		47  |   selector:
		48  |     matchLabels:
		49  |       app.kubernetes.io/name: cluster-upgrade
		50  |       app.kubernetes.io/instance: RELEASE-NAME
		51  |   template:
		52  |     metadata:
		53  |       labels:
		54  |         app.kubernetes.io/name: cluster-upgrade
		55  |         app.kubernetes.io/instance: RELEASE-NAME
		56  |       annotations:
		57  |         iam.amazonaws.com/role: "orvn--cluster-upgrade"
		58  |         ad.datadoghq.com/cluster-upgrade.check_names: |
		59  |           ["openmetrics"]
		60  |         ad.datadoghq.com/cluster-upgrade.init_configs: |
		61  |           [{}]
		62  |         ad.datadoghq.com/cluster-upgrade.instances: |
		63  |           [
		64  |             {
		65  |               "prometheus_url": "http://%%host%%:9404/metrics",
		66  |               "namespace": "cluster-upgrade",
		67  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		68  |             }
		69  |           ]
		70  |     spec:
		71  |       serviceAccountName: cluster-upgrade-sa
		72  |       securityContext:
		73  |         {}
		74  |       containers:
		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		35  | apiVersion: apps/v1
		36  | kind: Deployment
		37  | metadata:
		38  |   name: RELEASE-NAME-cluster-upgrade
		39  |   labels:
		40  |     app.kubernetes.io/name: cluster-upgrade
		41  |     helm.sh/chart: cluster-upgrade-0.209505439.4
		42  |     app.kubernetes.io/instance: RELEASE-NAME
		43  |     app.kubernetes.io/version: "1.0"
		44  |     app.kubernetes.io/managed-by: Helm
		45  | spec:
		46  |   replicas: 1
		47  |   selector:
		48  |     matchLabels:
		49  |       app.kubernetes.io/name: cluster-upgrade
		50  |       app.kubernetes.io/instance: RELEASE-NAME
		51  |   template:
		52  |     metadata:
		53  |       labels:
		54  |         app.kubernetes.io/name: cluster-upgrade
		55  |         app.kubernetes.io/instance: RELEASE-NAME
		56  |       annotations:
		57  |         iam.amazonaws.com/role: "orvn--cluster-upgrade"
		58  |         ad.datadoghq.com/cluster-upgrade.check_names: |
		59  |           ["openmetrics"]
		60  |         ad.datadoghq.com/cluster-upgrade.init_configs: |
		61  |           [{}]
		62  |         ad.datadoghq.com/cluster-upgrade.instances: |
		63  |           [
		64  |             {
		65  |               "prometheus_url": "http://%%host%%:9404/metrics",
		66  |               "namespace": "cluster-upgrade",
		67  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		68  |             }
		69  |           ]
		70  |     spec:
		71  |       serviceAccountName: cluster-upgrade-sa
		72  |       securityContext:
		73  |         {}
		74  |       containers:
		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default
	File: ./charts/cluster-upgrade.yaml:35-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		35  | apiVersion: apps/v1
		36  | kind: Deployment
		37  | metadata:
		38  |   name: RELEASE-NAME-cluster-upgrade
		39  |   labels:
		40  |     app.kubernetes.io/name: cluster-upgrade
		41  |     helm.sh/chart: cluster-upgrade-0.209505439.4
		42  |     app.kubernetes.io/instance: RELEASE-NAME
		43  |     app.kubernetes.io/version: "1.0"
		44  |     app.kubernetes.io/managed-by: Helm
		45  | spec:
		46  |   replicas: 1
		47  |   selector:
		48  |     matchLabels:
		49  |       app.kubernetes.io/name: cluster-upgrade
		50  |       app.kubernetes.io/instance: RELEASE-NAME
		51  |   template:
		52  |     metadata:
		53  |       labels:
		54  |         app.kubernetes.io/name: cluster-upgrade
		55  |         app.kubernetes.io/instance: RELEASE-NAME
		56  |       annotations:
		57  |         iam.amazonaws.com/role: "orvn--cluster-upgrade"
		58  |         ad.datadoghq.com/cluster-upgrade.check_names: |
		59  |           ["openmetrics"]
		60  |         ad.datadoghq.com/cluster-upgrade.init_configs: |
		61  |           [{}]
		62  |         ad.datadoghq.com/cluster-upgrade.instances: |
		63  |           [
		64  |             {
		65  |               "prometheus_url": "http://%%host%%:9404/metrics",
		66  |               "namespace": "cluster-upgrade",
		67  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		68  |             }
		69  |           ]
		70  |     spec:
		71  |       serviceAccountName: cluster-upgrade-sa
		72  |       securityContext:
		73  |         {}
		74  |       containers:
		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		152 | apiVersion: batch/v1beta1
		153 | kind: CronJob
		154 | 
		155 | metadata:
		156 |   name: cluster-upgrade-tls-refresh
		157 |   labels:
		158 |     app.kubernetes.io/name: cluster-upgrade
		159 |     helm.sh/chart: cluster-upgrade-0.209505439.4
		160 |     app.kubernetes.io/instance: RELEASE-NAME
		161 |     app.kubernetes.io/version: "1.0"
		162 |     app.kubernetes.io/managed-by: Helm
		163 | spec:
		164 |   schedule: "0 0 * * *"
		165 |   jobTemplate:
		166 |     spec:
		167 |       template:
		168 |         metadata:
		169 |           labels:
		170 |             app.kubernetes.io/name: cluster-upgrade
		171 |             app.kubernetes.io/instance: RELEASE-NAME
		172 |         spec:
		173 |           containers:
		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent
		201 |           restartPolicy: OnFailure
		202 |           serviceAccountName: cluster-upgrade-sa


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		152 | apiVersion: batch/v1beta1
		153 | kind: CronJob
		154 | 
		155 | metadata:
		156 |   name: cluster-upgrade-tls-refresh
		157 |   labels:
		158 |     app.kubernetes.io/name: cluster-upgrade
		159 |     helm.sh/chart: cluster-upgrade-0.209505439.4
		160 |     app.kubernetes.io/instance: RELEASE-NAME
		161 |     app.kubernetes.io/version: "1.0"
		162 |     app.kubernetes.io/managed-by: Helm
		163 | spec:
		164 |   schedule: "0 0 * * *"
		165 |   jobTemplate:
		166 |     spec:
		167 |       template:
		168 |         metadata:
		169 |           labels:
		170 |             app.kubernetes.io/name: cluster-upgrade
		171 |             app.kubernetes.io/instance: RELEASE-NAME
		172 |         spec:
		173 |           containers:
		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent
		201 |           restartPolicy: OnFailure
		202 |           serviceAccountName: cluster-upgrade-sa


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		152 | apiVersion: batch/v1beta1
		153 | kind: CronJob
		154 | 
		155 | metadata:
		156 |   name: cluster-upgrade-tls-refresh
		157 |   labels:
		158 |     app.kubernetes.io/name: cluster-upgrade
		159 |     helm.sh/chart: cluster-upgrade-0.209505439.4
		160 |     app.kubernetes.io/instance: RELEASE-NAME
		161 |     app.kubernetes.io/version: "1.0"
		162 |     app.kubernetes.io/managed-by: Helm
		163 | spec:
		164 |   schedule: "0 0 * * *"
		165 |   jobTemplate:
		166 |     spec:
		167 |       template:
		168 |         metadata:
		169 |           labels:
		170 |             app.kubernetes.io/name: cluster-upgrade
		171 |             app.kubernetes.io/instance: RELEASE-NAME
		172 |         spec:
		173 |           containers:
		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent
		201 |           restartPolicy: OnFailure
		202 |           serviceAccountName: cluster-upgrade-sa


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		152 | apiVersion: batch/v1beta1
		153 | kind: CronJob
		154 | 
		155 | metadata:
		156 |   name: cluster-upgrade-tls-refresh
		157 |   labels:
		158 |     app.kubernetes.io/name: cluster-upgrade
		159 |     helm.sh/chart: cluster-upgrade-0.209505439.4
		160 |     app.kubernetes.io/instance: RELEASE-NAME
		161 |     app.kubernetes.io/version: "1.0"
		162 |     app.kubernetes.io/managed-by: Helm
		163 | spec:
		164 |   schedule: "0 0 * * *"
		165 |   jobTemplate:
		166 |     spec:
		167 |       template:
		168 |         metadata:
		169 |           labels:
		170 |             app.kubernetes.io/name: cluster-upgrade
		171 |             app.kubernetes.io/instance: RELEASE-NAME
		172 |         spec:
		173 |           containers:
		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent
		201 |           restartPolicy: OnFailure
		202 |           serviceAccountName: cluster-upgrade-sa


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		152 | apiVersion: batch/v1beta1
		153 | kind: CronJob
		154 | 
		155 | metadata:
		156 |   name: cluster-upgrade-tls-refresh
		157 |   labels:
		158 |     app.kubernetes.io/name: cluster-upgrade
		159 |     helm.sh/chart: cluster-upgrade-0.209505439.4
		160 |     app.kubernetes.io/instance: RELEASE-NAME
		161 |     app.kubernetes.io/version: "1.0"
		162 |     app.kubernetes.io/managed-by: Helm
		163 | spec:
		164 |   schedule: "0 0 * * *"
		165 |   jobTemplate:
		166 |     spec:
		167 |       template:
		168 |         metadata:
		169 |           labels:
		170 |             app.kubernetes.io/name: cluster-upgrade
		171 |             app.kubernetes.io/instance: RELEASE-NAME
		172 |         spec:
		173 |           containers:
		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent
		201 |           restartPolicy: OnFailure
		202 |           serviceAccountName: cluster-upgrade-sa


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default
	File: ./charts/cluster-upgrade.yaml:152-202
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		152 | apiVersion: batch/v1beta1
		153 | kind: CronJob
		154 | 
		155 | metadata:
		156 |   name: cluster-upgrade-tls-refresh
		157 |   labels:
		158 |     app.kubernetes.io/name: cluster-upgrade
		159 |     helm.sh/chart: cluster-upgrade-0.209505439.4
		160 |     app.kubernetes.io/instance: RELEASE-NAME
		161 |     app.kubernetes.io/version: "1.0"
		162 |     app.kubernetes.io/managed-by: Helm
		163 | spec:
		164 |   schedule: "0 0 * * *"
		165 |   jobTemplate:
		166 |     spec:
		167 |       template:
		168 |         metadata:
		169 |           labels:
		170 |             app.kubernetes.io/name: cluster-upgrade
		171 |             app.kubernetes.io/instance: RELEASE-NAME
		172 |         spec:
		173 |           containers:
		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent
		201 |           restartPolicy: OnFailure
		202 |           serviceAccountName: cluster-upgrade-sa


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-cluster-upgrade.default (container 0) - cluster-upgrade
	File: ./charts/cluster-upgrade.yaml:75-149
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		75  |         - name: cluster-upgrade
		76  |           securityContext:
		77  |             {}
		78  |           image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		79  |           imagePullPolicy: IfNotPresent
		80  |           ports:
		81  |             - name: http
		82  |               containerPort: 80
		83  |               protocol: TCP
		84  |           livenessProbe:
		85  |             initialDelaySeconds: 15
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             initialDelaySeconds: 15
		91  |             httpGet:
		92  |               path: /actuator/health
		93  |               port: 8080
		94  |           resources:
		95  |             limits:
		96  |               memory: 2Gi
		97  |             requests:
		98  |               memory: 2Gi
		99  |           env:
		100 |             - name: SPRING_PROFILES_ACTIVE
		101 |               value: default, prod
		102 |               # These values come from the environment of the current container and are set during initial install
		103 |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		104 |               value: 
		105 |             - name: MANAGEMENT_SERVER_PORT
		106 |               value: "8080"
		107 |             
		108 |             
		109 |             
		110 |             - name: COOKIE_SECRET
		111 |               value: ''
		112 |             # Carries the current FRONTEND_CLIENT_ID forward during upgrades when the pod is recreated
		113 |             - name: FRONTEND_CLIENT_ID
		114 |               value: 
		115 |               # Carries the current GROUP_ID forward during upgrades when the pod is recreated
		116 |             - name: GROUP_ID
		117 |               value: 
		118 |             - name: SEGMENT_ANALYTICS_WRITEKEY
		119 |               value: "mLZedQqPeRGL19unziVuoVQXDXjMl94F"
		120 |               # Carries the current SERVICE_CLIENT_ID forward during upgrades when the pod is recreated
		121 |             - name: SERVICE_CLIENT_ID
		122 |               value: 
		123 |               # Carries the current SERVICE_CLIENT_SECRET forward during upgrades when the pod is recreated
		124 |             - name: SERVICE_CLIENT_SECRET
		125 |               value: 
		126 |               # Carries the current OPENRAVEN_INGRESS_HOSTNAME forward during upgrades when the pod is recreated
		127 |             - name: OPENRAVEN_INGRESS_HOSTNAME
		128 |               value: 
		129 |               # The service application's client id, used to auth to asgard
		130 |             - name: ADMIN_CLIENT_ID
		131 |               value: 
		132 |               # The service application's secret, used to auth to asgard
		133 |             - name: ADMIN_CLIENT_SECRET
		134 |               value: 
		135 |               # The name of the cluster selected during provisioning
		136 |             - name: CLUSTER_NAME
		137 |               value: 
		138 |             - name: SENTRY_DSN
		139 |               value: "https://54e5f8625c5e4fdba66c69eb47bf9e1a@o322024.ingest.sentry.io/5367691"
		140 |             - name: SENTRY_EXTRA
		141 |               value: "groupId:"
		142 |             - name: SENTRY_RELEASE
		143 |               value: "0.209505439.4"
		144 |             - name: JAVA_TOOL_OPTIONS
		145 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		146 |             - name: INSTALL_EMAIL
		147 |               value: 
		148 |             - name: CLUSTER_TYPE
		149 |               value:


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: CronJob.cluster-upgrade-tls-refresh.default (container 0) - refresh
	File: ./charts/cluster-upgrade.yaml:174-200
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		174 |           - name: refresh
		175 |             image: "registry.gitlab.com/openraven/open/cluster-upgrade:209505439"
		176 |             command:
		177 |             - bash
		178 |             - -exc
		179 |             # language=sh
		180 |             - |
		181 |               out_fn=cluster-tls.json
		182 |               crt_fn=www-tls.crt
		183 |               key_fn=www-tls.key
		184 |               curl -fsSLo $out_fn $ASGARD_URL/api/account/activate/tls/$GROUP_ID
		185 |               # jq 1.5 does not have "|@base64d"
		186 |               jq -r .tlsCertificateB64 < $out_fn | base64 --decode > $crt_fn
		187 |               jq -r .tlsPrivateKeyB64  < $out_fn | base64 --decode > $key_fn
		188 |               # we need the dry-run in order to use "replace" below
		189 |               kubectl -n $TLS_NAMESPACE create secret tls www-tls \
		190 |                   --cert=${crt_fn} --key=${key_fn} \
		191 |                   --dry-run -o json > www-tls.secret.json
		192 |               exec kubectl -n $TLS_NAMESPACE replace -f www-tls.secret.json
		193 |             env:
		194 |             - name: ASGARD_URL
		195 |               value: "https://api.openraven.com"
		196 |             - name: GROUP_ID
		197 |               value: 
		198 |             - name: TLS_NAMESPACE
		199 |               value: ui
		200 |             imagePullPolicy: IfNotPresent



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 30, Failed checks: 34, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_15: "Image Pull Policy should be Always"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-cross-account
		7  |   labels:
		8  |     app.kubernetes.io/name: cross-account
		9  |     helm.sh/chart: cross-account-0.253231769.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: cross-account
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cross-account
		29  |   labels:
		30  |     app.kubernetes.io/name: cross-account
		31  |     helm.sh/chart: cross-account-0.253231769.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cross-account
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cross-account
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cross-account.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cross-account.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cross-account.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "cross-account",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       serviceAccountName: default
		62  |       securityContext:
		63  |         {}
		64  |       containers:
		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cross-account
		29  |   labels:
		30  |     app.kubernetes.io/name: cross-account
		31  |     helm.sh/chart: cross-account-0.253231769.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cross-account
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cross-account
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cross-account.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cross-account.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cross-account.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "cross-account",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       serviceAccountName: default
		62  |       securityContext:
		63  |         {}
		64  |       containers:
		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cross-account
		29  |   labels:
		30  |     app.kubernetes.io/name: cross-account
		31  |     helm.sh/chart: cross-account-0.253231769.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cross-account
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cross-account
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cross-account.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cross-account.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cross-account.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "cross-account",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       serviceAccountName: default
		62  |       securityContext:
		63  |         {}
		64  |       containers:
		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cross-account
		29  |   labels:
		30  |     app.kubernetes.io/name: cross-account
		31  |     helm.sh/chart: cross-account-0.253231769.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cross-account
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cross-account
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cross-account.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cross-account.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cross-account.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "cross-account",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       serviceAccountName: default
		62  |       securityContext:
		63  |         {}
		64  |       containers:
		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:25-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-cross-account
		29  |   labels:
		30  |     app.kubernetes.io/name: cross-account
		31  |     helm.sh/chart: cross-account-0.253231769.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: cross-account
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: cross-account
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/cross-account.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/cross-account.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/cross-account.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "cross-account",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       serviceAccountName: default
		62  |       securityContext:
		63  |         {}
		64  |       containers:
		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-cross-account.default
	File: ./charts/cross-account.yaml:114-139
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		114 | apiVersion: networking.k8s.io/v1beta1
		115 | kind: Ingress
		116 | metadata:
		117 |   name: RELEASE-NAME-cross-account
		118 |   labels:
		119 |     app.kubernetes.io/name: cross-account
		120 |     helm.sh/chart: cross-account-0.253231769.0
		121 |     app.kubernetes.io/instance: RELEASE-NAME
		122 |     app.kubernetes.io/version: "1.0"
		123 |     app.kubernetes.io/managed-by: Helm
		124 |   annotations:
		125 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		126 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		127 |       proxy_set_header cookie "";
		128 |       auth_request_set $auth_cookie $upstream_http_set_cookie;
		129 |       add_header Set-Cookie $auth_cookie;
		130 |     nginx.ingress.kubernetes.io/proxy-read-timeout: "600"
		131 | spec:
		132 |   rules:
		133 |     - host: 
		134 |       http:
		135 |         paths:
		136 |           - path: /accounts/cross-account
		137 |             backend:
		138 |               serviceName: RELEASE-NAME-cross-account
		139 |               servicePort: 80


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		142 | apiVersion: v1
		143 | kind: Pod
		144 | metadata:
		145 |   name: "RELEASE-NAME-cross-account-test-connection"
		146 |   labels:
		147 |     app.kubernetes.io/name: cross-account
		148 |     helm.sh/chart: cross-account-0.253231769.0
		149 |     app.kubernetes.io/instance: RELEASE-NAME
		150 |     app.kubernetes.io/version: "1.0"
		151 |     app.kubernetes.io/managed-by: Helm
		152 |   annotations:
		153 |     "helm.sh/hook": test-success
		154 | spec:
		155 |   containers:
		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']
		160 |   restartPolicy: Never


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		142 | apiVersion: v1
		143 | kind: Pod
		144 | metadata:
		145 |   name: "RELEASE-NAME-cross-account-test-connection"
		146 |   labels:
		147 |     app.kubernetes.io/name: cross-account
		148 |     helm.sh/chart: cross-account-0.253231769.0
		149 |     app.kubernetes.io/instance: RELEASE-NAME
		150 |     app.kubernetes.io/version: "1.0"
		151 |     app.kubernetes.io/managed-by: Helm
		152 |   annotations:
		153 |     "helm.sh/hook": test-success
		154 | spec:
		155 |   containers:
		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']
		160 |   restartPolicy: Never


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		142 | apiVersion: v1
		143 | kind: Pod
		144 | metadata:
		145 |   name: "RELEASE-NAME-cross-account-test-connection"
		146 |   labels:
		147 |     app.kubernetes.io/name: cross-account
		148 |     helm.sh/chart: cross-account-0.253231769.0
		149 |     app.kubernetes.io/instance: RELEASE-NAME
		150 |     app.kubernetes.io/version: "1.0"
		151 |     app.kubernetes.io/managed-by: Helm
		152 |   annotations:
		153 |     "helm.sh/hook": test-success
		154 | spec:
		155 |   containers:
		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']
		160 |   restartPolicy: Never


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		142 | apiVersion: v1
		143 | kind: Pod
		144 | metadata:
		145 |   name: "RELEASE-NAME-cross-account-test-connection"
		146 |   labels:
		147 |     app.kubernetes.io/name: cross-account
		148 |     helm.sh/chart: cross-account-0.253231769.0
		149 |     app.kubernetes.io/instance: RELEASE-NAME
		150 |     app.kubernetes.io/version: "1.0"
		151 |     app.kubernetes.io/managed-by: Helm
		152 |   annotations:
		153 |     "helm.sh/hook": test-success
		154 | spec:
		155 |   containers:
		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']
		160 |   restartPolicy: Never


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		142 | apiVersion: v1
		143 | kind: Pod
		144 | metadata:
		145 |   name: "RELEASE-NAME-cross-account-test-connection"
		146 |   labels:
		147 |     app.kubernetes.io/name: cross-account
		148 |     helm.sh/chart: cross-account-0.253231769.0
		149 |     app.kubernetes.io/instance: RELEASE-NAME
		150 |     app.kubernetes.io/version: "1.0"
		151 |     app.kubernetes.io/managed-by: Helm
		152 |   annotations:
		153 |     "helm.sh/hook": test-success
		154 | spec:
		155 |   containers:
		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']
		160 |   restartPolicy: Never


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default
	File: ./charts/cross-account.yaml:142-160
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		142 | apiVersion: v1
		143 | kind: Pod
		144 | metadata:
		145 |   name: "RELEASE-NAME-cross-account-test-connection"
		146 |   labels:
		147 |     app.kubernetes.io/name: cross-account
		148 |     helm.sh/chart: cross-account-0.253231769.0
		149 |     app.kubernetes.io/instance: RELEASE-NAME
		150 |     app.kubernetes.io/version: "1.0"
		151 |     app.kubernetes.io/managed-by: Helm
		152 |   annotations:
		153 |     "helm.sh/hook": test-success
		154 | spec:
		155 |   containers:
		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']
		160 |   restartPolicy: Never


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-cross-account.default (container 0) - cross-account
	File: ./charts/cross-account.yaml:65-111
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		65  |         - name: cross-account
		66  |           securityContext:
		67  |             {}
		68  |           image: "registry.gitlab.com/openraven/open/cross-account:253231769"
		69  |           imagePullPolicy: IfNotPresent
		70  |           ports:
		71  |             - name: http
		72  |               containerPort: 80
		73  |               protocol: TCP
		74  |           livenessProbe:
		75  |             initialDelaySeconds: 15
		76  |             httpGet:
		77  |               path: /actuator/health
		78  |               port: 8080
		79  |           readinessProbe:
		80  |             initialDelaySeconds: 15
		81  |             httpGet:
		82  |               path: /actuator/health
		83  |               port: 8080
		84  |           resources:
		85  |             limits:
		86  |               memory: 1G
		87  |             requests:
		88  |               memory: 1G
		89  |           env:
		90  |             - name: SPRING_PROFILES_ACTIVE
		91  |               value: default, prod
		92  |             - name: MANAGEMENT_SERVER_PORT
		93  |               value: "8080"
		94  |             - name: SERVER_PORT
		95  |               value: "80"
		96  |             - name: SERVER_MAX-HTTP-HEADER-SIZE
		97  |               value: "128KB"
		98  |             - name: OPENRAVEN_APP_V1_CLOUD-INGESTION_ANALYTICS_CLUSTERID
		99  |               value: 
		100 |             - name: SENTRY_DSN
		101 |               value: "https://49533685193541daacc31a0c86e6bb11@o322024.ingest.sentry.io/5264893"
		102 |             - name: SENTRY_EXTRA
		103 |               value: "groupId:"
		104 |             - name: SENTRY_RELEASE
		105 |               value: "0.253231769.0"
		106 |             - name: SERVICE_CLIENT_ID
		107 |               value: 
		108 |             - name: SERVICE_CLIENT_SECRET
		109 |               value: 
		110 |             - name: JAVA_TOOL_OPTIONS
		111 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=90


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Pod.RELEASE-NAME-cross-account-test-connection.default (container 0) - wget
	File: ./charts/cross-account.yaml:156-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		156 |     - name: wget
		157 |       image: busybox
		158 |       command: ['wget']
		159 |       args:  ['RELEASE-NAME-cross-account:80']



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 


       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 28, Failed checks: 37, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_15: "Image Pull Policy should be Always"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-dmap
		7  |   labels:
		8  |     app.kubernetes.io/name: dmap
		9  |     helm.sh/chart: dmap-0.270854898.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "0.0.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: 80
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: dmap
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-dmap
		29  |   labels:
		30  |     app.kubernetes.io/name: dmap
		31  |     helm.sh/chart: dmap-0.270854898.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: dmap
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: dmap
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/dmap.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/dmap.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/dmap.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "dmap",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       containers:
		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-dmap
		29  |   labels:
		30  |     app.kubernetes.io/name: dmap
		31  |     helm.sh/chart: dmap-0.270854898.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: dmap
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: dmap
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/dmap.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/dmap.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/dmap.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "dmap",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       containers:
		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-dmap
		29  |   labels:
		30  |     app.kubernetes.io/name: dmap
		31  |     helm.sh/chart: dmap-0.270854898.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: dmap
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: dmap
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/dmap.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/dmap.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/dmap.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "dmap",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       containers:
		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-dmap
		29  |   labels:
		30  |     app.kubernetes.io/name: dmap
		31  |     helm.sh/chart: dmap-0.270854898.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: dmap
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: dmap
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/dmap.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/dmap.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/dmap.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "dmap",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       containers:
		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-dmap
		29  |   labels:
		30  |     app.kubernetes.io/name: dmap
		31  |     helm.sh/chart: dmap-0.270854898.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: dmap
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: dmap
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/dmap.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/dmap.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/dmap.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "dmap",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       containers:
		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:25-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-dmap
		29  |   labels:
		30  |     app.kubernetes.io/name: dmap
		31  |     helm.sh/chart: dmap-0.270854898.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: dmap
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: dmap
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/dmap.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/dmap.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/dmap.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:9404/metrics",
		55  |               "namespace": "dmap",
		56  |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		57  |             }
		58  |           ]
		59  |     spec:
		60  |       containers:
		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-dmap-authed.default
	File: ./charts/dmap.yaml:103-126
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		103 | apiVersion: networking.k8s.io/v1beta1
		104 | kind: Ingress
		105 | metadata:
		106 |   name: RELEASE-NAME-dmap-authed
		107 |   labels:
		108 |     app.kubernetes.io/name: dmap
		109 |     helm.sh/chart: dmap-0.270854898.0
		110 |     app.kubernetes.io/instance: RELEASE-NAME
		111 |     app.kubernetes.io/version: "0.0.0"
		112 |     app.kubernetes.io/managed-by: Helm
		113 |   annotations:
		114 |     nginx.ingress.kubernetes.io/auth-response-headers: x-auth-request-user, x-auth-request-email, authorization, x-auth-request-access-token
		115 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		116 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		117 |       proxy_set_header cookie "";
		118 | spec:
		119 |   rules:
		120 |     - host: 
		121 |       http:
		122 |         paths:
		123 |           - path: /api/dmap
		124 |             backend:
		125 |               serviceName: RELEASE-NAME-dmap
		126 |               servicePort: http


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-dmap.default
	File: ./charts/dmap.yaml:129-147
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		129 | apiVersion: networking.k8s.io/v1beta1
		130 | kind: Ingress
		131 | metadata:
		132 |   name: RELEASE-NAME-dmap
		133 |   labels:
		134 |     app.kubernetes.io/name: dmap
		135 |     helm.sh/chart: dmap-0.270854898.0
		136 |     app.kubernetes.io/instance: RELEASE-NAME
		137 |     app.kubernetes.io/version: "0.0.0"
		138 |     app.kubernetes.io/managed-by: Helm
		139 | spec:
		140 |   rules:
		141 |     - host: 
		142 |       http:
		143 |         paths:
		144 |           - path: /api/dmap/scanner
		145 |             backend:
		146 |               serviceName: RELEASE-NAME-dmap
		147 |               servicePort: http


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		150 | apiVersion: v1
		151 | kind: Pod
		152 | metadata:
		153 |   name: "RELEASE-NAME-dmap-test-connection"
		154 |   labels:
		155 |     app.kubernetes.io/name: dmap
		156 |     helm.sh/chart: dmap-0.270854898.0
		157 |     app.kubernetes.io/instance: RELEASE-NAME
		158 |     app.kubernetes.io/version: "0.0.0"
		159 |     app.kubernetes.io/managed-by: Helm
		160 |   annotations:
		161 |     "helm.sh/hook": test-success
		162 | spec:
		163 |   containers:
		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']
		168 |   restartPolicy: Never


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		150 | apiVersion: v1
		151 | kind: Pod
		152 | metadata:
		153 |   name: "RELEASE-NAME-dmap-test-connection"
		154 |   labels:
		155 |     app.kubernetes.io/name: dmap
		156 |     helm.sh/chart: dmap-0.270854898.0
		157 |     app.kubernetes.io/instance: RELEASE-NAME
		158 |     app.kubernetes.io/version: "0.0.0"
		159 |     app.kubernetes.io/managed-by: Helm
		160 |   annotations:
		161 |     "helm.sh/hook": test-success
		162 | spec:
		163 |   containers:
		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']
		168 |   restartPolicy: Never


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		150 | apiVersion: v1
		151 | kind: Pod
		152 | metadata:
		153 |   name: "RELEASE-NAME-dmap-test-connection"
		154 |   labels:
		155 |     app.kubernetes.io/name: dmap
		156 |     helm.sh/chart: dmap-0.270854898.0
		157 |     app.kubernetes.io/instance: RELEASE-NAME
		158 |     app.kubernetes.io/version: "0.0.0"
		159 |     app.kubernetes.io/managed-by: Helm
		160 |   annotations:
		161 |     "helm.sh/hook": test-success
		162 | spec:
		163 |   containers:
		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']
		168 |   restartPolicy: Never


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		150 | apiVersion: v1
		151 | kind: Pod
		152 | metadata:
		153 |   name: "RELEASE-NAME-dmap-test-connection"
		154 |   labels:
		155 |     app.kubernetes.io/name: dmap
		156 |     helm.sh/chart: dmap-0.270854898.0
		157 |     app.kubernetes.io/instance: RELEASE-NAME
		158 |     app.kubernetes.io/version: "0.0.0"
		159 |     app.kubernetes.io/managed-by: Helm
		160 |   annotations:
		161 |     "helm.sh/hook": test-success
		162 | spec:
		163 |   containers:
		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']
		168 |   restartPolicy: Never


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		150 | apiVersion: v1
		151 | kind: Pod
		152 | metadata:
		153 |   name: "RELEASE-NAME-dmap-test-connection"
		154 |   labels:
		155 |     app.kubernetes.io/name: dmap
		156 |     helm.sh/chart: dmap-0.270854898.0
		157 |     app.kubernetes.io/instance: RELEASE-NAME
		158 |     app.kubernetes.io/version: "0.0.0"
		159 |     app.kubernetes.io/managed-by: Helm
		160 |   annotations:
		161 |     "helm.sh/hook": test-success
		162 | spec:
		163 |   containers:
		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']
		168 |   restartPolicy: Never


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default
	File: ./charts/dmap.yaml:150-168
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		150 | apiVersion: v1
		151 | kind: Pod
		152 | metadata:
		153 |   name: "RELEASE-NAME-dmap-test-connection"
		154 |   labels:
		155 |     app.kubernetes.io/name: dmap
		156 |     helm.sh/chart: dmap-0.270854898.0
		157 |     app.kubernetes.io/instance: RELEASE-NAME
		158 |     app.kubernetes.io/version: "0.0.0"
		159 |     app.kubernetes.io/managed-by: Helm
		160 |   annotations:
		161 |     "helm.sh/hook": test-success
		162 | spec:
		163 |   containers:
		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']
		168 |   restartPolicy: Never


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-dmap.default (container 0) - dmap
	File: ./charts/dmap.yaml:61-100
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		61  |         - name: dmap
		62  |           
		63  |           image: "registry.gitlab.com/openraven/open/dmap:270854898"
		64  |           imagePullPolicy: IfNotPresent
		65  |           ports:
		66  |             - name: http
		67  |               containerPort: 80
		68  |               protocol: TCP
		69  |           livenessProbe:
		70  |             httpGet:
		71  |               path: /actuator/health
		72  |               port: 8080
		73  |           readinessProbe:
		74  |             httpGet:
		75  |               path: /actuator/health
		76  |               port: 8080
		77  |           env:
		78  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-ID
		79  |               value: 
		80  |             - name: OPENRAVEN_APP_V1_DMAP-ML_ADMIN-CLIENT-SECRET
		81  |               value: 
		82  |             - name: SPRING_PROFILES_ACTIVE
		83  |               value: prod
		84  |             - name: MANAGEMENT_SERVER_PORT
		85  |               value: '8080'
		86  |             - name: JAVA_TOOL_OPTIONS
		87  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		88  |             - name: SENTRY_DSN
		89  |               value: "https://d557df44b5df4057bac234d40efbadde@o322024.ingest.sentry.io/5379389"
		90  |             - name: SENTRY_ENVIRONMENT
		91  |               value: 
		92  |             - name: SENTRY_RELEASE
		93  |               value: "0.270854898.0"
		94  |             - name: SENTRY_EXTRA
		95  |               value: "groupId:"
		96  |           resources:
		97  |             limits:
		98  |               memory: 2Gi
		99  |             requests:
		100 |               memory: 2Gi


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Pod.RELEASE-NAME-dmap-test-connection.default (container 0) - wget
	File: ./charts/dmap.yaml:164-167
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		164 |     - name: wget
		165 |       image: busybox
		166 |       command: ['wget']
		167 |       args:  ['RELEASE-NAME-dmap:80']



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 18, Failed checks: 13, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-dmap-scheduler
		7  |   labels:
		8  |     app.kubernetes.io/name: dmap-scheduler
		9  |     helm.sh/chart: dmap-scheduler-0.268015721.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: dmap-scheduler
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: dmap-scheduler
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/dmap-scheduler.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/dmap-scheduler.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/dmap-scheduler.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:9404/metrics",
		33 |               "namespace": "dmap-scheduler",
		34 |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		35 |             }
		36 |           ]
		37 |         iam.amazonaws.com/role: "orvn--dmap-scheduler"
		38 |     spec:
		39 |       serviceAccountName: default
		40 |       securityContext:
		41 |         {}
		42 |       containers:
		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-dmap-scheduler
		7  |   labels:
		8  |     app.kubernetes.io/name: dmap-scheduler
		9  |     helm.sh/chart: dmap-scheduler-0.268015721.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: dmap-scheduler
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: dmap-scheduler
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/dmap-scheduler.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/dmap-scheduler.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/dmap-scheduler.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:9404/metrics",
		33 |               "namespace": "dmap-scheduler",
		34 |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		35 |             }
		36 |           ]
		37 |         iam.amazonaws.com/role: "orvn--dmap-scheduler"
		38 |     spec:
		39 |       serviceAccountName: default
		40 |       securityContext:
		41 |         {}
		42 |       containers:
		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-dmap-scheduler
		7  |   labels:
		8  |     app.kubernetes.io/name: dmap-scheduler
		9  |     helm.sh/chart: dmap-scheduler-0.268015721.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: dmap-scheduler
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: dmap-scheduler
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/dmap-scheduler.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/dmap-scheduler.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/dmap-scheduler.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:9404/metrics",
		33 |               "namespace": "dmap-scheduler",
		34 |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		35 |             }
		36 |           ]
		37 |         iam.amazonaws.com/role: "orvn--dmap-scheduler"
		38 |     spec:
		39 |       serviceAccountName: default
		40 |       securityContext:
		41 |         {}
		42 |       containers:
		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-dmap-scheduler
		7  |   labels:
		8  |     app.kubernetes.io/name: dmap-scheduler
		9  |     helm.sh/chart: dmap-scheduler-0.268015721.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: dmap-scheduler
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: dmap-scheduler
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/dmap-scheduler.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/dmap-scheduler.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/dmap-scheduler.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:9404/metrics",
		33 |               "namespace": "dmap-scheduler",
		34 |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		35 |             }
		36 |           ]
		37 |         iam.amazonaws.com/role: "orvn--dmap-scheduler"
		38 |     spec:
		39 |       serviceAccountName: default
		40 |       securityContext:
		41 |         {}
		42 |       containers:
		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default
	File: ./charts/dmap-scheduler.yaml:3-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-dmap-scheduler
		7  |   labels:
		8  |     app.kubernetes.io/name: dmap-scheduler
		9  |     helm.sh/chart: dmap-scheduler-0.268015721.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: dmap-scheduler
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: dmap-scheduler
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/dmap-scheduler.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/dmap-scheduler.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/dmap-scheduler.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:9404/metrics",
		33 |               "namespace": "dmap-scheduler",
		34 |               "metrics": ["java_*", "jvm_*", "kafka_*", "process_*"]
		35 |             }
		36 |           ]
		37 |         iam.amazonaws.com/role: "orvn--dmap-scheduler"
		38 |     spec:
		39 |       serviceAccountName: default
		40 |       securityContext:
		41 |         {}
		42 |       containers:
		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-dmap-scheduler.default (container 0) - dmap-scheduler
	File: ./charts/dmap-scheduler.yaml:43-89
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		43 |         - name: dmap-scheduler
		44 |           securityContext:
		45 |             {}
		46 |           image: "registry.gitlab.com/openraven/open/dmap-scheduler-repo-docker:268015721"
		47 |           imagePullPolicy: IfNotPresent
		48 |           ports:
		49 |             - name: http
		50 |               containerPort: 80
		51 |               protocol: TCP
		52 |           livenessProbe:
		53 |             httpGet:
		54 |               path: /actuator/health
		55 |               port: 8080
		56 |           readinessProbe:
		57 |             httpGet:
		58 |               path: /actuator/health
		59 |               port: 8080
		60 |           resources:
		61 |             limits:
		62 |               memory: 2Gi
		63 |             requests:
		64 |               memory: 2Gi
		65 |           env:
		66 |             - name: SPRING_PROFILES_ACTIVE
		67 |               value: default,prod,dmap,s3
		68 |             - name: SERVER_PORT
		69 |               value: "80"
		70 |             - name: MANAGEMENT_SERVER_PORT
		71 |               value: "8080"
		72 |             - name: JAVA_TOOL_OPTIONS
		73 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		74 |             - name: SENTRY_DSN
		75 |               value: "https://092daf12f2d241dda7249f61fcfbebb6@o322024.ingest.sentry.io/5379408"
		76 |             - name: SENTRY_ENVIRONMENT
		77 |               value: 
		78 |             - name: SENTRY_RELEASE
		79 |               value: "0.268015721.0"
		80 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_VERSION
		81 |               value: 
		82 |             - name: OPENRAVEN_APP_V1_DMAP_SCHEDULING_DEPLOYCHANNEL
		83 |               value: 
		84 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_VERSION
		85 |               value: 
		86 |             - name: OPENRAVEN_APP_V1_S3_SCHEDULING_DEPLOYCHANNEL
		87 |               value: 
		88 |             - name: SENTRY_EXTRA
		89 |               value: "groupId:"



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 35, Failed checks: 29, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_11: "CPU limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

Check: CKV_K8S_10: "CPU requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-elastic-search-node-client
		7  |   labels:
		8  |     app.kubernetes.io/name: elastic-search-node-client
		9  |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: elastic-search-node-client
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-elastic-search-node-client
		29 |   labels:
		30 |     app.kubernetes.io/name: elastic-search-node-client
		31 |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: elastic-search-node-client
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: elastic-search-node-client
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       serviceAccountName: default
		48 |       securityContext:
		49 |         {}
		50 |       containers:
		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-elastic-search-node-client
		29 |   labels:
		30 |     app.kubernetes.io/name: elastic-search-node-client
		31 |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: elastic-search-node-client
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: elastic-search-node-client
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       serviceAccountName: default
		48 |       securityContext:
		49 |         {}
		50 |       containers:
		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-elastic-search-node-client
		29 |   labels:
		30 |     app.kubernetes.io/name: elastic-search-node-client
		31 |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: elastic-search-node-client
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: elastic-search-node-client
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       serviceAccountName: default
		48 |       securityContext:
		49 |         {}
		50 |       containers:
		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-elastic-search-node-client
		29 |   labels:
		30 |     app.kubernetes.io/name: elastic-search-node-client
		31 |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: elastic-search-node-client
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: elastic-search-node-client
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       serviceAccountName: default
		48 |       securityContext:
		49 |         {}
		50 |       containers:
		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:25-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-elastic-search-node-client
		29 |   labels:
		30 |     app.kubernetes.io/name: elastic-search-node-client
		31 |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: elastic-search-node-client
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: elastic-search-node-client
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       serviceAccountName: default
		48 |       securityContext:
		49 |         {}
		50 |       containers:
		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		88  | apiVersion: batch/v1
		89  | kind: Job
		90  | metadata:
		91  |   name: RELEASE-NAME-elastic-search-node-client-imports
		92  |   labels:
		93  |     app.kubernetes.io/name: elastic-search-node-client
		94  |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		95  |     app.kubernetes.io/instance: RELEASE-NAME
		96  |     app.kubernetes.io/version: "1.0"
		97  |     app.kubernetes.io/managed-by: Helm
		98  | spec:
		99  |   # Yes this is ridiculous, but we need to REALLY try hard to get this into the cluster
		100 |   backoffLimit: 25
		101 |   template:
		102 |     metadata:
		103 |       labels:
		104 |         app.kubernetes.io/name: elastic-search-node-client-imports
		105 |         app.kubernetes.io/instance: RELEASE-NAME
		106 |     spec:
		107 |       serviceAccountName: default
		108 |       securityContext:
		109 |         {}
		110 |       restartPolicy: OnFailure
		111 |       
		112 |       
		113 |       containers:
		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		88  | apiVersion: batch/v1
		89  | kind: Job
		90  | metadata:
		91  |   name: RELEASE-NAME-elastic-search-node-client-imports
		92  |   labels:
		93  |     app.kubernetes.io/name: elastic-search-node-client
		94  |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		95  |     app.kubernetes.io/instance: RELEASE-NAME
		96  |     app.kubernetes.io/version: "1.0"
		97  |     app.kubernetes.io/managed-by: Helm
		98  | spec:
		99  |   # Yes this is ridiculous, but we need to REALLY try hard to get this into the cluster
		100 |   backoffLimit: 25
		101 |   template:
		102 |     metadata:
		103 |       labels:
		104 |         app.kubernetes.io/name: elastic-search-node-client-imports
		105 |         app.kubernetes.io/instance: RELEASE-NAME
		106 |     spec:
		107 |       serviceAccountName: default
		108 |       securityContext:
		109 |         {}
		110 |       restartPolicy: OnFailure
		111 |       
		112 |       
		113 |       containers:
		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		88  | apiVersion: batch/v1
		89  | kind: Job
		90  | metadata:
		91  |   name: RELEASE-NAME-elastic-search-node-client-imports
		92  |   labels:
		93  |     app.kubernetes.io/name: elastic-search-node-client
		94  |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		95  |     app.kubernetes.io/instance: RELEASE-NAME
		96  |     app.kubernetes.io/version: "1.0"
		97  |     app.kubernetes.io/managed-by: Helm
		98  | spec:
		99  |   # Yes this is ridiculous, but we need to REALLY try hard to get this into the cluster
		100 |   backoffLimit: 25
		101 |   template:
		102 |     metadata:
		103 |       labels:
		104 |         app.kubernetes.io/name: elastic-search-node-client-imports
		105 |         app.kubernetes.io/instance: RELEASE-NAME
		106 |     spec:
		107 |       serviceAccountName: default
		108 |       securityContext:
		109 |         {}
		110 |       restartPolicy: OnFailure
		111 |       
		112 |       
		113 |       containers:
		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		88  | apiVersion: batch/v1
		89  | kind: Job
		90  | metadata:
		91  |   name: RELEASE-NAME-elastic-search-node-client-imports
		92  |   labels:
		93  |     app.kubernetes.io/name: elastic-search-node-client
		94  |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		95  |     app.kubernetes.io/instance: RELEASE-NAME
		96  |     app.kubernetes.io/version: "1.0"
		97  |     app.kubernetes.io/managed-by: Helm
		98  | spec:
		99  |   # Yes this is ridiculous, but we need to REALLY try hard to get this into the cluster
		100 |   backoffLimit: 25
		101 |   template:
		102 |     metadata:
		103 |       labels:
		104 |         app.kubernetes.io/name: elastic-search-node-client-imports
		105 |         app.kubernetes.io/instance: RELEASE-NAME
		106 |     spec:
		107 |       serviceAccountName: default
		108 |       securityContext:
		109 |         {}
		110 |       restartPolicy: OnFailure
		111 |       
		112 |       
		113 |       containers:
		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default
	File: ./charts/elastic-search-node-client.yaml:88-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		88  | apiVersion: batch/v1
		89  | kind: Job
		90  | metadata:
		91  |   name: RELEASE-NAME-elastic-search-node-client-imports
		92  |   labels:
		93  |     app.kubernetes.io/name: elastic-search-node-client
		94  |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		95  |     app.kubernetes.io/instance: RELEASE-NAME
		96  |     app.kubernetes.io/version: "1.0"
		97  |     app.kubernetes.io/managed-by: Helm
		98  | spec:
		99  |   # Yes this is ridiculous, but we need to REALLY try hard to get this into the cluster
		100 |   backoffLimit: 25
		101 |   template:
		102 |     metadata:
		103 |       labels:
		104 |         app.kubernetes.io/name: elastic-search-node-client-imports
		105 |         app.kubernetes.io/instance: RELEASE-NAME
		106 |     spec:
		107 |       serviceAccountName: default
		108 |       securityContext:
		109 |         {}
		110 |       restartPolicy: OnFailure
		111 |       
		112 |       
		113 |       containers:
		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-elastic-search-node-client.default
	File: ./charts/elastic-search-node-client.yaml:166-189
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		166 | apiVersion: networking.k8s.io/v1beta1
		167 | kind: Ingress
		168 | metadata:
		169 |   name: RELEASE-NAME-elastic-search-node-client
		170 |   labels:
		171 |     app.kubernetes.io/name: elastic-search-node-client
		172 |     helm.sh/chart: elastic-search-node-client-0.217604975.1
		173 |     app.kubernetes.io/instance: RELEASE-NAME
		174 |     app.kubernetes.io/version: "1.0"
		175 |     app.kubernetes.io/managed-by: Helm
		176 |   annotations:
		177 |     nginx.ingress.kubernetes.io/auth-response-headers: x-auth-request-user, x-auth-request-email, authorization, x-auth-request-access-token
		178 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		179 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		180 |       proxy_set_header cookie "";
		181 | spec:
		182 |   rules:
		183 |     - host: 
		184 |       http:
		185 |         paths:
		186 |           - path: /es
		187 |             backend:
		188 |               serviceName: RELEASE-NAME-elastic-search-node-client
		189 |               servicePort: 80


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-elastic-search-node-client.default (container 0) - elastic-search-node-client
	File: ./charts/elastic-search-node-client.yaml:51-85
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		51 |         - name: elastic-search-node-client
		52 |           securityContext:
		53 |             {}
		54 |           image: "registry.gitlab.com/openraven/open/elastic-search-node-client:217604975"
		55 |           imagePullPolicy: IfNotPresent
		56 |           env:
		57 |             - name: ELASTIC_URL
		58 |               value: http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200
		59 |             - name: SENTRY_DSN
		60 |               value: "https://7188f75192984a8bbc36a212ab341b2c@o322024.ingest.sentry.io/5270430"
		61 |             - name: SENTRY_ENVIRONMENT
		62 |               value: 
		63 |             - name: SENTRY_RELEASE
		64 |               value: "0.217604975.1"
		65 |             - name: SENTRY_EXTRA
		66 |               value: "groupId:"
		67 |           ports:
		68 |             - name: http
		69 |               containerPort: 3000
		70 |               protocol: TCP
		71 |           livenessProbe:
		72 |             httpGet:
		73 |               path: /es/ping
		74 |               port: http
		75 |           readinessProbe:
		76 |             httpGet:
		77 |               path: /es/ping
		78 |               port: http
		79 |           resources:
		80 |             limits:
		81 |               cpu: 500m
		82 |               memory: 512Mi
		83 |             requests:
		84 |               cpu: 200m
		85 |               memory: 512Mi


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Job.RELEASE-NAME-elastic-search-node-client-imports.default (container 0) - elasticdump-imports
	File: ./charts/elastic-search-node-client.yaml:114-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		114 |       - name: elasticdump-imports
		115 |         image: elasticdump/elasticsearch-dump:v6.28.4
		116 |         imagePullPolicy: IfNotPresent
		117 |         command:
		118 |           - /bin/sh
		119 |           - -exc
		120 |           # language=sh
		121 |           - |
		122 |             # yes, even 10 minutes may not be enough for ES to be a-ok
		123 |             # and realistically we want this to wait forever, but we'll start here
		124 |             for _ in $(seq 1 60); do
		125 |               if wget --quiet -O /dev/null -T 1 http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/_cat/health >/dev/null 2>&1; then
		126 |                 break
		127 |               fi
		128 |               sleep 10
		129 |             done
		130 |             wget https://.s3.amazonaws.com/es/configtemplate.json
		131 | 
		132 |             elasticdump \
		133 |               --input=./configtemplate.json \
		134 |               --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/configservice \
		135 |               --type=template
		136 | 
		137 |             demodb="$(wget --quiet -O - http://account-management.ui.svc.cluster.local/api/server/settings/demodb)"
		138 |             if [ "$demodb" != "false" ]; then
		139 |               wget https://.s3.amazonaws.com/es/demodb.json
		140 |               elasticdump \
		141 |                 --input=./demodb.json \
		142 |                 --output=http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/ \
		143 |                 --type=data
		144 | 
		145 |               # Explicitly waiting until after the elasticdump to do this work as it is not as critical as the above
		146 |               # Add the aliases needed for the index pattern in kibana
		147 |               apk add curl
		148 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets'
		149 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_first_added'
		150 |               curl -X PUT -fsSL 'http://elasticsearch-es-http.elasticsearch.svc.cluster.local:9200/aws*/_alias/or_assets_last_updated'
		151 | 
		152 |               out_fn=dashboards.ndjson
		153 |               curl -fsSLo ${out_fn} https://.s3.amazonaws.com/es/$out_fn
		154 |               for _ in 1 2 3 4 5 6 7 8 9 10; do
		155 |                 if curl -fsSL -H "kbn-xsrf: true" \
		156 |                        --form "file=@${out_fn}" \
		157 |                       "http://kibana-kb-http.elasticsearch.svc.cluster.local:5601/kibana/api/saved_objects/_import?overwrite=true"
		158 |                 then
		159 |                   break
		160 |                 fi
		161 |                 sleep 30 # 10 seconds wasnt enough
		162 |               done
		163 |             fi



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 19, Failed checks: 25, Skipped checks: 0

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_11: "CPU limits should be set"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

Check: CKV_K8S_10: "CPU requests should be set"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_11: "CPU limits should be set"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.elasticsearch.default
	File: ./charts/elasticsearch.yaml:3-23
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: networking.k8s.io/v1beta1
		4  | kind: Ingress
		5  | metadata:
		6  |   name: "elasticsearch"
		7  |   annotations:
		8  |     ingress.kubernetes.io/proxy-body-size: 50m
		9  |     ingress.kubernetes.io/proxy-request-buffering: "off"
		10 |     kubernetes.io/ingress.class: nginx
		11 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		12 |     nginx.ingress.kubernetes.io/proxy-body-size: 50m
		13 |     nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
		14 |     nginx.ingress.kubernetes.io/rewrite-target: /$2
		15 | spec:
		16 |   rules:
		17 |     - host: 
		18 |       http:
		19 |         paths:
		20 |           - path: "/elasticsearch(/|$)(.*)"
		21 |             backend:
		22 |               serviceName: elasticsearch-es-http
		23 |               servicePort: 9200


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.kibana.default
	File: ./charts/elasticsearch.yaml:26-45
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		26 | apiVersion: networking.k8s.io/v1beta1
		27 | kind: Ingress
		28 | metadata:
		29 |   name: "kibana"
		30 |   annotations:
		31 |     ingress.kubernetes.io/proxy-body-size: 50m
		32 |     ingress.kubernetes.io/proxy-request-buffering: "off"
		33 |     kubernetes.io/ingress.class: nginx
		34 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		35 |     nginx.ingress.kubernetes.io/proxy-body-size: 50m
		36 |     nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
		37 | spec:
		38 |   rules:
		39 |     - host: 
		40 |       http:
		41 |         paths:
		42 |         - path: "/kibana"
		43 |           backend:
		44 |             serviceName: kibana-kb-http
		45 |             servicePort: 5601


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Elasticsearch.elasticsearch.default (container 0) - elasticsearch
	File: ./charts/elasticsearch.yaml:92-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		92  |           - env:
		93  |               - name: ES_JAVA_OPTS
		94  |                 # MUST BE 1/2 of the memory below
		95  |                 value: -Xms2g -Xmx2g
		96  |             name: elasticsearch
		97  |             resources:
		98  |               limits:
		99  |                 cpu: "1"
		100 |                 memory: 4Gi
		101 |               requests:
		102 |                 cpu: "1"
		103 |                 memory: 4Gi


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Kibana.kibana.default (container 0) - kibana
	File: ./charts/elasticsearch.yaml:149-163
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		149 |       - name: "kibana"
		150 |         resources:
		151 |           limits:
		152 |             memory: 2Gi
		153 |             cpu: 1
		154 |         readinessProbe:
		155 |           failureThreshold: 3
		156 |           httpGet:
		157 |             path: "/kibana"
		158 |             port: 5601
		159 |             scheme: HTTP
		160 |           initialDelaySeconds: 10
		161 |           periodSeconds: 10
		162 |           successThreshold: 1
		163 |           timeoutSeconds: 5



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 13, Failed checks: 18, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_19: "Containers should not share the host network namespace"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default
	File: ./charts/etcd-prune.yaml:3-159
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

		3   | apiVersion: apps/v1
		4   | kind: Deployment
		5   | metadata:
		6   |   name: RELEASE-NAME-etcd-prune
		7   |   labels:
		8   |     helm.sh/chart: etcd-prune-0.1.0
		9   |     app.kubernetes.io/name: etcd-prune
		10  |     app.kubernetes.io/instance: RELEASE-NAME
		11  |     app.kubernetes.io/version: "1.0.0"
		12  |     app.kubernetes.io/managed-by: Helm
		13  | spec:
		14  |   replicas: 2
		15  |   selector:
		16  |     matchLabels:
		17  |       app.kubernetes.io/name: etcd-prune
		18  |       app.kubernetes.io/instance: RELEASE-NAME
		19  |   template:
		20  |     metadata:
		21  |       labels:
		22  |         app.kubernetes.io/name: etcd-prune
		23  |         app.kubernetes.io/instance: RELEASE-NAME
		24  |     spec:
		25  |       containers:
		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true
		141 |       volumes:
		142 |       - hostPath:
		143 |           path: /opt
		144 |           type: Directory
		145 |         name: opt
		146 |       - hostPath:
		147 |           path: /etc/kubernetes/pki/etcd
		148 |           type: Directory
		149 |         name: pki-etcd
		150 |       hostNetwork: true
		151 |       nodeSelector:
		152 |         node-role.kubernetes.io/master: ""
		153 |       affinity:
		154 |         podAntiAffinity:
		155 |           requiredDuringSchedulingIgnoredDuringExecution:
		156 |           - topologyKey: kubernetes.io/hostname
		157 |       tolerations:
		158 |         - effect: NoSchedule
		159 |           operator: Exists


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-etcd-prune.default (container 0) - etcd-prune
	File: ./charts/etcd-prune.yaml:26-140
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		26  |         - name: etcd-prune
		27  |           image: "docker.io/library/python:3.8"
		28  |           imagePullPolicy: IfNotPresent
		29  |           resources:
		30  |             limits:
		31  |               memory: 128Mi
		32  |             requests:
		33  |               memory: 64Mi
		34  |           command:
		35  |           - /bin/bash
		36  |           - -ec
		37  |           # language=sh
		38  |           - |
		39  |             SLEEP_SECS=$(expr "$HOST_IP" : '.*\([0-9]\)$' || true)
		40  |             # we want to keep two prunes from running simultaneously
		41  |             export SLEEP_SECS
		42  |             echo "$WATCH_SH" > etcd_watch.sh
		43  |             chmod 755          etcd_watch.sh
		44  |             unset WATCH_SH
		45  |             echo "$PRUNE_SH" > etcd_prune.sh
		46  |             chmod 755          etcd_prune.sh
		47  |             unset PRUNE_SH
		48  |             PATH=$PATH:/opt/bin
		49  |             export ETCDCTL_API
		50  |             export ETCDCTL_CACERT
		51  |             export ETCDCTL_CERT
		52  |             export ETCDCTL_KEY
		53  |             # etcdctl does not pass along these vars into the subprocess
		54  |             # (perhaps understandably) so we have to manually make them available
		55  |             printenv | sed -ne '/^ETCDCTL_/s/^/export /p' > etcdctl.env
		56  |             # the "grep Key" is to keep etcdctl from dumping the WATCH_KEY to stdout
		57  |             # as it's still in binary (the JSON output is better, but huge)
		58  |             /opt/bin/etcdctl watch --prefix=true --write-out=simple /registry/minions -- ./etcd_watch.sh | grep Key
		59  |           env:
		60  |           # this makes the yaml quoting a ton easier
		61  |           - name: WATCH_SH
		62  |             # language=sh
		63  |             value: |
		64  |               #! /usr/bin/env bash
		65  |               doit=0
		66  |               if [[ "$ETCD_WATCH_EVENT_TYPE" == '"DELETE"' ]]; then
		67  |                  set &>/tmp/cp-delete-$$.log
		68  |                  for _ in 1 2 3 4 5; do
		69  |                    echo "Key change \"$ETCD_WATCH_KEY\" is a DELETE type" >&2
		70  |                  done
		71  |                  doit=1
		72  |               fi
		73  |               if expr "$ETCD_WATCH_VALUE" : '.*kubernetes[.]io/master' &>/dev/null; then
		74  |                  set &>/tmp/cp-set-$$.log
		75  |                  echo "Key change \"$ETCD_WATCH_KEY\" affects a control plane instance" >&2
		76  |                  doit=1
		77  |               fi
		78  |               if [[ $doit -eq 1 ]]; then
		79  |                   SLEEP_SECS=${SLEEP_SECS:-1}
		80  |                   echo "Running prune due to matching key criteria after $SLEEP_SECS seconds" >&2
		81  |                   sleep $SLEEP_SECS
		82  |                   exec ./etcd_prune.sh
		83  |               fi
		84  |           - name: PRUNE_SH
		85  |             # language=sh
		86  |             value: |
		87  |               #! /usr/bin/env bash
		88  |               PATH=$PATH:/opt/bin
		89  |               . ./etcdctl.env
		90  |               get_endpoints() {
		91  |                 etcdctl --write-out=json member list | python -c '
		92  |               import json
		93  |               import sys
		94  |               data = json.load(sys.stdin)
		95  |               print(",".join([m["clientURLs"][0] for m in data["members"]]))
		96  |               '
		97  |               }
		98  |               get_member_id_by_client_url() {
		99  |                 etcdctl --write-out=json member list | python -c '
		100 |               import json
		101 |               import re
		102 |               import sys
		103 |               client_url = sys.argv[1]
		104 |               data = json.load(sys.stdin)
		105 |               print("".join([re.sub(r"^0x", "", hex(m["ID"]))
		106 |                     for m in data["members"] if client_url in m["clientURLs"]]))
		107 |               ' "$1"
		108 |               }
		109 |               get_unhealthy_urls() {
		110 |                 etcdctl endpoint health 2>&1 | awk '/is unhealthy/{print $1}'
		111 |               }
		112 |               export ETCDCTL_ENDPOINTS="$(get_endpoints)"
		113 |               echo "== endpoint health" >&2
		114 |               if ! etcdctl endpoint health >&2; then
		115 |                 echo "BOGUS: unable to sniff out endpoint health of \"$ETCDCTL_ENDPOINTS\"" >&2
		116 |               fi
		117 |               echo "==/endpoint health" >&2
		118 |               for ep in $(get_unhealthy_urls); do
		119 |                 m_id=$(get_member_id_by_client_url "$ep")
		120 |                 etcdctl member remove "$m_id" >&2
		121 |               done
		122 |           - name: ETCDCTL_API
		123 |             value: "3"
		124 |           - name: ETCDCTL_CACERT
		125 |             value: /etc/kubernetes/pki/etcd/ca.crt
		126 |           - name: ETCDCTL_CERT
		127 |             value: /etc/kubernetes/pki/etcd/server.crt
		128 |           - name: ETCDCTL_KEY
		129 |             value: /etc/kubernetes/pki/etcd/server.key
		130 |           - name: HOST_IP
		131 |             valueFrom:
		132 |               fieldRef:
		133 |                 fieldPath: status.hostIP
		134 |           volumeMounts:
		135 |           - mountPath: /opt
		136 |             name: opt
		137 |             readOnly: true
		138 |           - mountPath: /etc/kubernetes/pki/etcd
		139 |             name: pki-etcd
		140 |             readOnly: true



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 16, Failed checks: 17, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-integrations
		7  |   labels:
		8  |     app.kubernetes.io/name: integrations
		9  |     helm.sh/chart: integrations-0.261953786.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "0.0.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: 80
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: integrations
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-integrations
		29  |   labels:
		30  |     app.kubernetes.io/name: integrations
		31  |     helm.sh/chart: integrations-0.261953786.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: integrations
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: integrations
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/integrations.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/integrations.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/integrations.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "integrations",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       containers:
		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-integrations
		29  |   labels:
		30  |     app.kubernetes.io/name: integrations
		31  |     helm.sh/chart: integrations-0.261953786.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: integrations
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: integrations
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/integrations.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/integrations.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/integrations.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "integrations",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       containers:
		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-integrations
		29  |   labels:
		30  |     app.kubernetes.io/name: integrations
		31  |     helm.sh/chart: integrations-0.261953786.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: integrations
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: integrations
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/integrations.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/integrations.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/integrations.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "integrations",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       containers:
		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-integrations
		29  |   labels:
		30  |     app.kubernetes.io/name: integrations
		31  |     helm.sh/chart: integrations-0.261953786.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: integrations
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: integrations
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/integrations.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/integrations.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/integrations.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "integrations",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       containers:
		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-integrations
		29  |   labels:
		30  |     app.kubernetes.io/name: integrations
		31  |     helm.sh/chart: integrations-0.261953786.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: integrations
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: integrations
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/integrations.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/integrations.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/integrations.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "integrations",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       containers:
		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-integrations
		29  |   labels:
		30  |     app.kubernetes.io/name: integrations
		31  |     helm.sh/chart: integrations-0.261953786.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: integrations
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: integrations
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/integrations.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/integrations.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/integrations.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "integrations",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--cross-account"
		60  |     spec:
		61  |       containers:
		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-integrations.default
	File: ./charts/integrations.yaml:106-124
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		106 | apiVersion: networking.k8s.io/v1beta1
		107 | kind: Ingress
		108 | metadata:
		109 |   name: RELEASE-NAME-integrations
		110 |   labels:
		111 |     app.kubernetes.io/name: integrations
		112 |     helm.sh/chart: integrations-0.261953786.0
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 | spec:
		117 |   rules:
		118 |     - host: 
		119 |       http:
		120 |         paths:
		121 |           - path: /api/integrations/test
		122 |             backend:
		123 |               serviceName: RELEASE-NAME-integrations
		124 |               servicePort: 80


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-integrations.default (container 0) - integrations
	File: ./charts/integrations.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		62  |         - name: integrations
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/integrations-service-repo-docker:261953786"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		84  |               value: 
		85  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		86  |               value: 
		87  |             - name: OPENRAVEN_APP_V1_SERVICES_SERVICE_CLUSTER-URL
		88  |               value: "https://"
		89  |             - name: JAVA_TOOL_OPTIONS
		90  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		91  |             - name: SENTRY_DSN
		92  |               value: "https://f7b2669a7437459aa7632ba39772d3d3@o322024.ingest.sentry.io/5563380"
		93  |             - name: SENTRY_ENVIRONMENT
		94  |               value: 
		95  |             - name: SENTRY_RELEASE
		96  |               value: "0.261953786.0"
		97  |             - name: SENTRY_EXTRA
		98  |               value: "groupId:"
		99  |           resources:
		100 |             limits:
		101 |               memory: 512Mi
		102 |             requests:
		103 |               memory: 512Mi



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 21, Failed checks: 12, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_11: "CPU limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

Check: CKV_K8S_10: "CPU requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-introspection
		7  |   labels:
		8  |     helm.sh/chart: introspection-0.1.1
		9  |     app.kubernetes.io/name: introspection
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "0.1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: introspection
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-introspection
		29 |   labels:
		30 |     helm.sh/chart: introspection-0.1.1
		31 |     app.kubernetes.io/name: introspection
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "0.1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: introspection
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: introspection
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       securityContext:
		48 |         {}
		49 |       containers:
		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi
		79 |       automountServiceAccountToken: false


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-introspection
		29 |   labels:
		30 |     helm.sh/chart: introspection-0.1.1
		31 |     app.kubernetes.io/name: introspection
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "0.1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: introspection
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: introspection
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       securityContext:
		48 |         {}
		49 |       containers:
		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi
		79 |       automountServiceAccountToken: false


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-introspection
		29 |   labels:
		30 |     helm.sh/chart: introspection-0.1.1
		31 |     app.kubernetes.io/name: introspection
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "0.1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: introspection
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: introspection
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       securityContext:
		48 |         {}
		49 |       containers:
		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi
		79 |       automountServiceAccountToken: false


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:25-79
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25 | apiVersion: apps/v1
		26 | kind: Deployment
		27 | metadata:
		28 |   name: RELEASE-NAME-introspection
		29 |   labels:
		30 |     helm.sh/chart: introspection-0.1.1
		31 |     app.kubernetes.io/name: introspection
		32 |     app.kubernetes.io/instance: RELEASE-NAME
		33 |     app.kubernetes.io/version: "0.1.0"
		34 |     app.kubernetes.io/managed-by: Helm
		35 | spec:
		36 |   replicas: 1
		37 |   selector:
		38 |     matchLabels:
		39 |       app.kubernetes.io/name: introspection
		40 |       app.kubernetes.io/instance: RELEASE-NAME
		41 |   template:
		42 |     metadata:
		43 |       labels:
		44 |         app.kubernetes.io/name: introspection
		45 |         app.kubernetes.io/instance: RELEASE-NAME
		46 |     spec:
		47 |       securityContext:
		48 |         {}
		49 |       containers:
		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi
		79 |       automountServiceAccountToken: false


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-introspection.default
	File: ./charts/introspection.yaml:82-105
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		82  | apiVersion: networking.k8s.io/v1beta1
		83  | kind: Ingress
		84  | metadata:
		85  |   name: RELEASE-NAME-introspection
		86  |   labels:
		87  |     helm.sh/chart: introspection-0.1.1
		88  |     app.kubernetes.io/name: introspection
		89  |     app.kubernetes.io/instance: RELEASE-NAME
		90  |     app.kubernetes.io/version: "0.1.0"
		91  |     app.kubernetes.io/managed-by: Helm
		92  |   annotations:
		93  |     nginx.ingress.kubernetes.io/auth-response-headers: x-auth-request-user, x-auth-request-email, authorization, x-auth-request-access-token
		94  |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		95  |     nginx.ingress.kubernetes.io/configuration-snippet: |
		96  |       proxy_set_header cookie "";
		97  | spec:
		98  |   rules:
		99  |     - host: 
		100 |       http:
		101 |         paths:
		102 |           - path: /explorer/introspection
		103 |             backend:
		104 |               serviceName: RELEASE-NAME-introspection
		105 |               servicePort: 80


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-introspection.default (container 0) - introspection
	File: ./charts/introspection.yaml:50-78
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		50 |         - name: introspection
		51 |           securityContext:
		52 |             {}
		53 |           image: "docker.io/library/python:3.8"
		54 |           imagePullPolicy: IfNotPresent
		55 |           args:
		56 |           - python
		57 |           - -u
		58 |           - -c
		59 |           -           "#! /usr/bin/env python\r\n# coding=utf-8\r\nimport json\r\nfrom http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer\r\n\r\n\r\nclass IntrospectionHandler(SimpleHTTPRequestHandler):\r\n\r\n    def do_GET(self) -> None:\r\n        if self.path == '/healthz':\r\n            # this is the non-logging version\r\n            self.send_response_only(200)\r\n            self.send_header('content-length', '0')\r\n            self.end_headers()\r\n            return\r\n        try:\r\n            results = {}\r\n            for header, value in self.headers.items():\r\n                # the original introspection endpoint sent back the headers\r\n                # in a List<String> so we (obviously) recreate that shape\r\n                results[header.lower()] = [value]\r\n            resp = json.dumps(results).encode('utf-8')\r\n            c_len = len(resp)\r\n            self.send_response(200)\r\n            self.send_header('content-length', str(c_len))\r\n            self.send_header('content-type', 'application/json;charset=utf-8')\r\n            self.end_headers()\r\n            self.wfile.write(resp)\r\n            self.wfile.flush()\r\n        except Exception as e:\r\n            self.log_error('Bogus: %s', str(e))\r\n            self.send_error(500)\r\n            self.send_header('content-length', '0')\r\n\r\n\r\nif __name__ == '__main__':\r\n    server_address = ('0.0.0.0', 80)\r\n    s = ThreadingHTTPServer(server_address, IntrospectionHandler)\r\n    # noinspection PyBroadException\r\n    try:\r\n        s.serve_forever()\r\n    except Exception:\r\n        s.shutdown()\r\n"
		60 |           ports:
		61 |             - name: http
		62 |               containerPort: 80
		63 |               protocol: TCP
		64 |           livenessProbe:
		65 |             httpGet:
		66 |               path: /healthz
		67 |               port: http
		68 |           readinessProbe:
		69 |             httpGet:
		70 |               path: /healthz
		71 |               port: http
		72 |           resources:
		73 |             limits:
		74 |               cpu: 100m
		75 |               memory: 128Mi
		76 |             requests:
		77 |               cpu: 100m
		78 |               memory: 128Mi



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 16, Failed checks: 22, Skipped checks: 0

Check: CKV_K8S_41: "Ensure that default service accounts are not actively used"
	PASSED for resource: ServiceAccount.logdna-agent.default
	File: ./charts/logdna-agent.yaml:3-9
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_38

Check: CKV_K8S_42: "Ensure that default service accounts are not actively used"
	PASSED for resource: ClusterRoleBinding.logdna-agent
	File: ./charts/logdna-agent.yaml:37-51
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_38

Check: CKV_K8S_42: "Ensure that default service accounts are not actively used"
	PASSED for resource: RoleBinding.logdna-agent.default
	File: ./charts/logdna-agent.yaml:67-81
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_38

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_10: "CPU requests should be set"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: ServiceAccount.logdna-agent.default
	File: ./charts/logdna-agent.yaml:3-9
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3 | apiVersion: v1
		4 | kind: ServiceAccount
		5 | metadata:
		6 |   name: logdna-agent
		7 |   labels:
		8 |     app.kubernetes.io/name: logdna-agent
		9 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Secret.logdna-agent-key.default
	File: ./charts/logdna-agent.yaml:12-18
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		12 | apiVersion: v1
		13 | kind: Secret
		14 | type: Opaque
		15 | data:
		16 |   logdna-agent-key: 
		17 | metadata:
		18 |   name: logdna-agent-key


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Role.logdna-agent.default
	File: ./charts/logdna-agent.yaml:54-64
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		54 | apiVersion: rbac.authorization.k8s.io/v1
		55 | kind: Role
		56 | metadata:
		57 |   name: logdna-agent
		58 |   labels:
		59 |     app.kubernetes.io/name: logdna-agent
		60 |     app.kubernetes.io/instance: RELEASE-NAME
		61 | rules:
		62 |   - apiGroups: [""]
		63 |     resources: ["configmaps"]
		64 |     verbs: ["get","list", "create", "watch"]


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: RoleBinding.logdna-agent.default
	File: ./charts/logdna-agent.yaml:67-81
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		67 | apiVersion: rbac.authorization.k8s.io/v1
		68 | kind: RoleBinding
		69 | metadata:
		70 |   name: logdna-agent
		71 |   labels:
		72 |     app.kubernetes.io/name: logdna-agent
		73 |     app.kubernetes.io/instance: RELEASE-NAME
		74 | roleRef:
		75 |   apiGroup: rbac.authorization.k8s.io
		76 |   kind: Role
		77 |   name: logdna-agent
		78 | subjects:
		79 |   - kind: ServiceAccount
		80 |     name: logdna-agent
		81 |     namespace: logdna-agent


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		84  | apiVersion: apps/v1
		85  | kind: DaemonSet
		86  | metadata:
		87  |   name: logdna-agent
		88  |   labels:
		89  |     app.kubernetes.io/name: logdna-agent
		90  |     helm.sh/chart: logdna-agent-0.1.1
		91  |     app.kubernetes.io/instance: RELEASE-NAME
		92  |     app.kubernetes.io/version: "1.0"
		93  |     app.kubernetes.io/managed-by: Helm
		94  | spec:
		95  |   updateStrategy:
		96  |     type: RollingUpdate
		97  |     rollingUpdate:
		98  |       maxUnavailable: 100%
		99  |   selector:
		100 |     matchLabels:
		101 |       app.kubernetes.io/name: logdna-agent
		102 |       app.kubernetes.io/instance: RELEASE-NAME
		103 |   template:
		104 |     metadata:
		105 |       labels:
		106 |         app.kubernetes.io/name: logdna-agent
		107 |         app.kubernetes.io/instance: RELEASE-NAME
		108 |     spec:
		109 |       serviceAccountName: logdna-agent
		110 |       containers:
		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true
		156 |       tolerations:
		157 |         - effect: NoSchedule
		158 |           operator: Exists
		159 |       volumes:
		160 |         - name: varlog
		161 |           hostPath:
		162 |             path: /var/log
		163 |         - name: vardata
		164 |           hostPath:
		165 |             path: /var/data
		166 |         - name: varlibdockercontainers
		167 |           hostPath:
		168 |             path: /var/lib/docker/containers
		169 |         - name: mnt
		170 |           hostPath:
		171 |             path: /mnt
		172 |         - name: osrelease
		173 |           hostPath:
		174 |             path: /etc/os-release
		175 |         - name: logdnahostname
		176 |           hostPath:
		177 |             path: /etc/hostname


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		84  | apiVersion: apps/v1
		85  | kind: DaemonSet
		86  | metadata:
		87  |   name: logdna-agent
		88  |   labels:
		89  |     app.kubernetes.io/name: logdna-agent
		90  |     helm.sh/chart: logdna-agent-0.1.1
		91  |     app.kubernetes.io/instance: RELEASE-NAME
		92  |     app.kubernetes.io/version: "1.0"
		93  |     app.kubernetes.io/managed-by: Helm
		94  | spec:
		95  |   updateStrategy:
		96  |     type: RollingUpdate
		97  |     rollingUpdate:
		98  |       maxUnavailable: 100%
		99  |   selector:
		100 |     matchLabels:
		101 |       app.kubernetes.io/name: logdna-agent
		102 |       app.kubernetes.io/instance: RELEASE-NAME
		103 |   template:
		104 |     metadata:
		105 |       labels:
		106 |         app.kubernetes.io/name: logdna-agent
		107 |         app.kubernetes.io/instance: RELEASE-NAME
		108 |     spec:
		109 |       serviceAccountName: logdna-agent
		110 |       containers:
		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true
		156 |       tolerations:
		157 |         - effect: NoSchedule
		158 |           operator: Exists
		159 |       volumes:
		160 |         - name: varlog
		161 |           hostPath:
		162 |             path: /var/log
		163 |         - name: vardata
		164 |           hostPath:
		165 |             path: /var/data
		166 |         - name: varlibdockercontainers
		167 |           hostPath:
		168 |             path: /var/lib/docker/containers
		169 |         - name: mnt
		170 |           hostPath:
		171 |             path: /mnt
		172 |         - name: osrelease
		173 |           hostPath:
		174 |             path: /etc/os-release
		175 |         - name: logdnahostname
		176 |           hostPath:
		177 |             path: /etc/hostname


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		84  | apiVersion: apps/v1
		85  | kind: DaemonSet
		86  | metadata:
		87  |   name: logdna-agent
		88  |   labels:
		89  |     app.kubernetes.io/name: logdna-agent
		90  |     helm.sh/chart: logdna-agent-0.1.1
		91  |     app.kubernetes.io/instance: RELEASE-NAME
		92  |     app.kubernetes.io/version: "1.0"
		93  |     app.kubernetes.io/managed-by: Helm
		94  | spec:
		95  |   updateStrategy:
		96  |     type: RollingUpdate
		97  |     rollingUpdate:
		98  |       maxUnavailable: 100%
		99  |   selector:
		100 |     matchLabels:
		101 |       app.kubernetes.io/name: logdna-agent
		102 |       app.kubernetes.io/instance: RELEASE-NAME
		103 |   template:
		104 |     metadata:
		105 |       labels:
		106 |         app.kubernetes.io/name: logdna-agent
		107 |         app.kubernetes.io/instance: RELEASE-NAME
		108 |     spec:
		109 |       serviceAccountName: logdna-agent
		110 |       containers:
		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true
		156 |       tolerations:
		157 |         - effect: NoSchedule
		158 |           operator: Exists
		159 |       volumes:
		160 |         - name: varlog
		161 |           hostPath:
		162 |             path: /var/log
		163 |         - name: vardata
		164 |           hostPath:
		165 |             path: /var/data
		166 |         - name: varlibdockercontainers
		167 |           hostPath:
		168 |             path: /var/lib/docker/containers
		169 |         - name: mnt
		170 |           hostPath:
		171 |             path: /mnt
		172 |         - name: osrelease
		173 |           hostPath:
		174 |             path: /etc/os-release
		175 |         - name: logdnahostname
		176 |           hostPath:
		177 |             path: /etc/hostname


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		84  | apiVersion: apps/v1
		85  | kind: DaemonSet
		86  | metadata:
		87  |   name: logdna-agent
		88  |   labels:
		89  |     app.kubernetes.io/name: logdna-agent
		90  |     helm.sh/chart: logdna-agent-0.1.1
		91  |     app.kubernetes.io/instance: RELEASE-NAME
		92  |     app.kubernetes.io/version: "1.0"
		93  |     app.kubernetes.io/managed-by: Helm
		94  | spec:
		95  |   updateStrategy:
		96  |     type: RollingUpdate
		97  |     rollingUpdate:
		98  |       maxUnavailable: 100%
		99  |   selector:
		100 |     matchLabels:
		101 |       app.kubernetes.io/name: logdna-agent
		102 |       app.kubernetes.io/instance: RELEASE-NAME
		103 |   template:
		104 |     metadata:
		105 |       labels:
		106 |         app.kubernetes.io/name: logdna-agent
		107 |         app.kubernetes.io/instance: RELEASE-NAME
		108 |     spec:
		109 |       serviceAccountName: logdna-agent
		110 |       containers:
		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true
		156 |       tolerations:
		157 |         - effect: NoSchedule
		158 |           operator: Exists
		159 |       volumes:
		160 |         - name: varlog
		161 |           hostPath:
		162 |             path: /var/log
		163 |         - name: vardata
		164 |           hostPath:
		165 |             path: /var/data
		166 |         - name: varlibdockercontainers
		167 |           hostPath:
		168 |             path: /var/lib/docker/containers
		169 |         - name: mnt
		170 |           hostPath:
		171 |             path: /mnt
		172 |         - name: osrelease
		173 |           hostPath:
		174 |             path: /etc/os-release
		175 |         - name: logdnahostname
		176 |           hostPath:
		177 |             path: /etc/hostname


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		84  | apiVersion: apps/v1
		85  | kind: DaemonSet
		86  | metadata:
		87  |   name: logdna-agent
		88  |   labels:
		89  |     app.kubernetes.io/name: logdna-agent
		90  |     helm.sh/chart: logdna-agent-0.1.1
		91  |     app.kubernetes.io/instance: RELEASE-NAME
		92  |     app.kubernetes.io/version: "1.0"
		93  |     app.kubernetes.io/managed-by: Helm
		94  | spec:
		95  |   updateStrategy:
		96  |     type: RollingUpdate
		97  |     rollingUpdate:
		98  |       maxUnavailable: 100%
		99  |   selector:
		100 |     matchLabels:
		101 |       app.kubernetes.io/name: logdna-agent
		102 |       app.kubernetes.io/instance: RELEASE-NAME
		103 |   template:
		104 |     metadata:
		105 |       labels:
		106 |         app.kubernetes.io/name: logdna-agent
		107 |         app.kubernetes.io/instance: RELEASE-NAME
		108 |     spec:
		109 |       serviceAccountName: logdna-agent
		110 |       containers:
		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true
		156 |       tolerations:
		157 |         - effect: NoSchedule
		158 |           operator: Exists
		159 |       volumes:
		160 |         - name: varlog
		161 |           hostPath:
		162 |             path: /var/log
		163 |         - name: vardata
		164 |           hostPath:
		165 |             path: /var/data
		166 |         - name: varlibdockercontainers
		167 |           hostPath:
		168 |             path: /var/lib/docker/containers
		169 |         - name: mnt
		170 |           hostPath:
		171 |             path: /mnt
		172 |         - name: osrelease
		173 |           hostPath:
		174 |             path: /etc/os-release
		175 |         - name: logdnahostname
		176 |           hostPath:
		177 |             path: /etc/hostname


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: DaemonSet.logdna-agent.default
	File: ./charts/logdna-agent.yaml:84-177
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		84  | apiVersion: apps/v1
		85  | kind: DaemonSet
		86  | metadata:
		87  |   name: logdna-agent
		88  |   labels:
		89  |     app.kubernetes.io/name: logdna-agent
		90  |     helm.sh/chart: logdna-agent-0.1.1
		91  |     app.kubernetes.io/instance: RELEASE-NAME
		92  |     app.kubernetes.io/version: "1.0"
		93  |     app.kubernetes.io/managed-by: Helm
		94  | spec:
		95  |   updateStrategy:
		96  |     type: RollingUpdate
		97  |     rollingUpdate:
		98  |       maxUnavailable: 100%
		99  |   selector:
		100 |     matchLabels:
		101 |       app.kubernetes.io/name: logdna-agent
		102 |       app.kubernetes.io/instance: RELEASE-NAME
		103 |   template:
		104 |     metadata:
		105 |       labels:
		106 |         app.kubernetes.io/name: logdna-agent
		107 |         app.kubernetes.io/instance: RELEASE-NAME
		108 |     spec:
		109 |       serviceAccountName: logdna-agent
		110 |       containers:
		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true
		156 |       tolerations:
		157 |         - effect: NoSchedule
		158 |           operator: Exists
		159 |       volumes:
		160 |         - name: varlog
		161 |           hostPath:
		162 |             path: /var/log
		163 |         - name: vardata
		164 |           hostPath:
		165 |             path: /var/data
		166 |         - name: varlibdockercontainers
		167 |           hostPath:
		168 |             path: /var/lib/docker/containers
		169 |         - name: mnt
		170 |           hostPath:
		171 |             path: /mnt
		172 |         - name: osrelease
		173 |           hostPath:
		174 |             path: /etc/os-release
		175 |         - name: logdnahostname
		176 |           hostPath:
		177 |             path: /etc/hostname


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true


Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	FAILED for resource: DaemonSet.logdna-agent.default (container 0) - logdna-agent
	File: ./charts/logdna-agent.yaml:111-155
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

		111 |         - name: logdna-agent
		112 |           image: "docker.io/logdna/logdna-agent:2.1.9"
		113 |           imagePullPolicy: IfNotPresent
		114 |           env:
		115 |             - name: LOGDNA_TAGS
		116 |               value: 
		117 |             - name: LOGDNA_AGENT_KEY
		118 |               valueFrom:
		119 |                 secretKeyRef:
		120 |                   name: logdna-agent-key
		121 |                   key: logdna-agent-key
		122 |             - name: RUST_LOG
		123 |               value: warn
		124 |             - name: NODE_NAME
		125 |               valueFrom:
		126 |                 fieldRef:
		127 |                   fieldPath: spec.nodeName
		128 |             - name: NAMESPACE
		129 |               valueFrom:
		130 |                 fieldRef:
		131 |                   fieldPath: metadata.namespace
		132 |           resources:
		133 |             requests:
		134 |               cpu: 20m
		135 |             limits:
		136 |               memory: 500Mi
		137 |           volumeMounts:
		138 |             - name: varlog
		139 |               mountPath: /var/log
		140 |               readOnly: true
		141 |             - name: vardata
		142 |               mountPath: /var/data
		143 |               readOnly: true
		144 |             - name: varlibdockercontainers
		145 |               mountPath: /var/lib/docker/containers
		146 |               readOnly: true
		147 |             - name: mnt
		148 |               mountPath: /mnt
		149 |               readOnly: true
		150 |             - name: osrelease
		151 |               mountPath: /etc/os-release
		152 |               readOnly: true
		153 |             - name: logdnahostname
		154 |               mountPath: /etc/logdna-hostname
		155 |               readOnly: true



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 18, Failed checks: 13, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-odin-metrics
		7  |   labels:
		8  |     app.kubernetes.io/name: odin-metrics
		9  |     helm.sh/chart: odin-metrics-0.252138541.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: odin-metrics
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: odin-metrics
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/odin-metrics.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/odin-metrics.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/odin-metrics.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		33 |               "namespace": "odin-metrics",
		34 |               "metrics": ["*"]
		35 |             }
		36 |           ]
		37 |     spec:
		38 |       serviceAccountName: default
		39 |       securityContext:
		40 |         {}
		41 |       containers:
		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-odin-metrics
		7  |   labels:
		8  |     app.kubernetes.io/name: odin-metrics
		9  |     helm.sh/chart: odin-metrics-0.252138541.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: odin-metrics
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: odin-metrics
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/odin-metrics.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/odin-metrics.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/odin-metrics.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		33 |               "namespace": "odin-metrics",
		34 |               "metrics": ["*"]
		35 |             }
		36 |           ]
		37 |     spec:
		38 |       serviceAccountName: default
		39 |       securityContext:
		40 |         {}
		41 |       containers:
		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-odin-metrics
		7  |   labels:
		8  |     app.kubernetes.io/name: odin-metrics
		9  |     helm.sh/chart: odin-metrics-0.252138541.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: odin-metrics
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: odin-metrics
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/odin-metrics.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/odin-metrics.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/odin-metrics.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		33 |               "namespace": "odin-metrics",
		34 |               "metrics": ["*"]
		35 |             }
		36 |           ]
		37 |     spec:
		38 |       serviceAccountName: default
		39 |       securityContext:
		40 |         {}
		41 |       containers:
		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-odin-metrics
		7  |   labels:
		8  |     app.kubernetes.io/name: odin-metrics
		9  |     helm.sh/chart: odin-metrics-0.252138541.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: odin-metrics
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: odin-metrics
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/odin-metrics.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/odin-metrics.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/odin-metrics.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		33 |               "namespace": "odin-metrics",
		34 |               "metrics": ["*"]
		35 |             }
		36 |           ]
		37 |     spec:
		38 |       serviceAccountName: default
		39 |       securityContext:
		40 |         {}
		41 |       containers:
		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default
	File: ./charts/odin-metrics.yaml:3-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		3  | apiVersion: apps/v1
		4  | kind: Deployment
		5  | metadata:
		6  |   name: RELEASE-NAME-odin-metrics
		7  |   labels:
		8  |     app.kubernetes.io/name: odin-metrics
		9  |     helm.sh/chart: odin-metrics-0.252138541.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   replicas: 1
		15 |   selector:
		16 |     matchLabels:
		17 |       app.kubernetes.io/name: odin-metrics
		18 |       app.kubernetes.io/instance: RELEASE-NAME
		19 |   template:
		20 |     metadata:
		21 |       labels:
		22 |         app.kubernetes.io/name: odin-metrics
		23 |         app.kubernetes.io/instance: RELEASE-NAME
		24 |       annotations:
		25 |         ad.datadoghq.com/odin-metrics.check_names: |
		26 |           ["openmetrics"]
		27 |         ad.datadoghq.com/odin-metrics.init_configs: |
		28 |           [{}]
		29 |         ad.datadoghq.com/odin-metrics.instances: |
		30 |           [
		31 |             {
		32 |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		33 |               "namespace": "odin-metrics",
		34 |               "metrics": ["*"]
		35 |             }
		36 |           ]
		37 |     spec:
		38 |       serviceAccountName: default
		39 |       securityContext:
		40 |         {}
		41 |       containers:
		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-odin-metrics.default (container 0) - odin-metrics
	File: ./charts/odin-metrics.yaml:42-84
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		42 |         - name: odin-metrics
		43 |           securityContext:
		44 |             {}
		45 |           image: "registry.gitlab.com/openraven/open/odin-metrics:252138541"
		46 |           imagePullPolicy: IfNotPresent
		47 |           livenessProbe:
		48 |             httpGet:
		49 |               path: /actuator/health
		50 |               port: 8080
		51 |           readinessProbe:
		52 |             httpGet:
		53 |               path: /actuator/health
		54 |               port: 8080
		55 |           resources:
		56 |             limits:
		57 |               memory: 512Mi
		58 |             requests:
		59 |               memory: 512Mi
		60 |           env:
		61 |             - name: SPRING_PROFILES_ACTIVE
		62 |               value: default,prod
		63 |             - name: SERVER_PORT
		64 |               value: "80"
		65 |             - name: MANAGEMENT_SERVER_PORT
		66 |               value: "8080"
		67 |             - name: JAVA_TOOL_OPTIONS
		68 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=50
		69 |             - name: SENTRY_DSN
		70 |               value: "https://21bd8b55b911434eaa1f727ef883b6bb@o322024.ingest.sentry.io/5551382"
		71 |             - name: SENTRY_ENVIRONMENT
		72 |               value: 
		73 |             - name: SENTRY_RELEASE
		74 |               value: "0.252138541.0"
		75 |             - name: SENTRY_EXTRA
		76 |               value: "groupId:"
		77 |             - name: CLUSTER_NAME
		78 |               value: 
		79 |             - name: CLUSTER_TYPE
		80 |               value: 
		81 |             - name: DEPLOY_CHANNEL
		82 |               value: 
		83 |             - name: GROUP_ID
		84 |               value:



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 31, Failed checks: 23, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_29: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_30: "Apply security context to your pods and containers"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-policy
		7  |   labels:
		8  |     app.kubernetes.io/name: policy
		9  |     helm.sh/chart: policy-0.260725765.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "1.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: http
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: policy
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-policy
		29  |   labels:
		30  |     app.kubernetes.io/name: policy
		31  |     helm.sh/chart: policy-0.260725765.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: policy
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: policy
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/policy.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/policy.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/policy.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "policy",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         ad.datadoghq.com/opa.check_names: |
		60  |           ["openmetrics"]
		61  |         ad.datadoghq.com/opa.init_configs: |
		62  |           [{}]
		63  |         ad.datadoghq.com/opa.instances: |
		64  |           [
		65  |             {
		66  |               "prometheus_url": "http://%%host%%:8082/metrics",
		67  |               "namespace": "opa",
		68  |               "metrics": ["*"]
		69  |             }
		70  |           ]
		71  |     spec:
		72  |       serviceAccountName: default
		73  |       securityContext:
		74  |         {}
		75  |       containers:
		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"
		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-policy
		29  |   labels:
		30  |     app.kubernetes.io/name: policy
		31  |     helm.sh/chart: policy-0.260725765.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: policy
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: policy
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/policy.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/policy.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/policy.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "policy",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         ad.datadoghq.com/opa.check_names: |
		60  |           ["openmetrics"]
		61  |         ad.datadoghq.com/opa.init_configs: |
		62  |           [{}]
		63  |         ad.datadoghq.com/opa.instances: |
		64  |           [
		65  |             {
		66  |               "prometheus_url": "http://%%host%%:8082/metrics",
		67  |               "namespace": "opa",
		68  |               "metrics": ["*"]
		69  |             }
		70  |           ]
		71  |     spec:
		72  |       serviceAccountName: default
		73  |       securityContext:
		74  |         {}
		75  |       containers:
		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"
		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-policy
		29  |   labels:
		30  |     app.kubernetes.io/name: policy
		31  |     helm.sh/chart: policy-0.260725765.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: policy
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: policy
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/policy.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/policy.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/policy.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "policy",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         ad.datadoghq.com/opa.check_names: |
		60  |           ["openmetrics"]
		61  |         ad.datadoghq.com/opa.init_configs: |
		62  |           [{}]
		63  |         ad.datadoghq.com/opa.instances: |
		64  |           [
		65  |             {
		66  |               "prometheus_url": "http://%%host%%:8082/metrics",
		67  |               "namespace": "opa",
		68  |               "metrics": ["*"]
		69  |             }
		70  |           ]
		71  |     spec:
		72  |       serviceAccountName: default
		73  |       securityContext:
		74  |         {}
		75  |       containers:
		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"
		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-policy
		29  |   labels:
		30  |     app.kubernetes.io/name: policy
		31  |     helm.sh/chart: policy-0.260725765.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: policy
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: policy
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/policy.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/policy.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/policy.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "policy",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         ad.datadoghq.com/opa.check_names: |
		60  |           ["openmetrics"]
		61  |         ad.datadoghq.com/opa.init_configs: |
		62  |           [{}]
		63  |         ad.datadoghq.com/opa.instances: |
		64  |           [
		65  |             {
		66  |               "prometheus_url": "http://%%host%%:8082/metrics",
		67  |               "namespace": "opa",
		68  |               "metrics": ["*"]
		69  |             }
		70  |           ]
		71  |     spec:
		72  |       serviceAccountName: default
		73  |       securityContext:
		74  |         {}
		75  |       containers:
		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"
		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:25-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-policy
		29  |   labels:
		30  |     app.kubernetes.io/name: policy
		31  |     helm.sh/chart: policy-0.260725765.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "1.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: policy
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: policy
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/policy.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/policy.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/policy.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "policy",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         ad.datadoghq.com/opa.check_names: |
		60  |           ["openmetrics"]
		61  |         ad.datadoghq.com/opa.init_configs: |
		62  |           [{}]
		63  |         ad.datadoghq.com/opa.instances: |
		64  |           [
		65  |             {
		66  |               "prometheus_url": "http://%%host%%:8082/metrics",
		67  |               "namespace": "opa",
		68  |               "metrics": ["*"]
		69  |             }
		70  |           ]
		71  |     spec:
		72  |       serviceAccountName: default
		73  |       securityContext:
		74  |         {}
		75  |       containers:
		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"
		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-policy.default
	File: ./charts/policy.yaml:149-194
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		149 | apiVersion: networking.k8s.io/v1beta1
		150 | kind: Ingress
		151 | metadata:
		152 |   name: RELEASE-NAME-policy
		153 |   labels:
		154 |     app.kubernetes.io/name: policy
		155 |     helm.sh/chart: policy-0.260725765.0
		156 |     app.kubernetes.io/instance: RELEASE-NAME
		157 |     app.kubernetes.io/version: "1.0"
		158 |     app.kubernetes.io/managed-by: Helm
		159 |   annotations:
		160 |     nginx.ingress.kubernetes.io/auth-response-headers: x-auth-request-user, x-auth-request-email, authorization, x-auth-request-access-token
		161 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		162 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		163 |       proxy_set_header cookie "";
		164 |       auth_request_set $auth_cookie $upstream_http_set_cookie;
		165 |       add_header Set-Cookie $auth_cookie;
		166 | spec:
		167 |   rules:
		168 |     - host: 
		169 |       http:
		170 |         paths:
		171 |           - path: /api/rules
		172 |             backend:
		173 |               serviceName: RELEASE-NAME-policy
		174 |               servicePort: 80
		175 |           - path: /api/policies
		176 |             backend:
		177 |               serviceName: RELEASE-NAME-policy
		178 |               servicePort: 80
		179 |           - path: /api/policyaudits
		180 |             backend:
		181 |               serviceName: RELEASE-NAME-policy
		182 |               servicePort: 80
		183 |           - path: /api/policy-rule
		184 |             backend:
		185 |               serviceName: RELEASE-NAME-policy
		186 |               servicePort: 80
		187 |           - path: /api/policy-reports
		188 |             backend:
		189 |               serviceName: RELEASE-NAME-policy
		190 |               servicePort: 80
		191 |           - path: /api/s3-bucket-details
		192 |             backend:
		193 |               serviceName: RELEASE-NAME-policy
		194 |               servicePort: 80


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 0) - policy
	File: ./charts/policy.yaml:76-116
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		76  |         - name: policy
		77  |           securityContext:
		78  |             {}
		79  |           image: "registry.gitlab.com/openraven/open/policy-service-repo-docker:260725765"
		80  |           imagePullPolicy: IfNotPresent
		81  |           ports:
		82  |             - name: http
		83  |               containerPort: 80
		84  |               protocol: TCP
		85  |           livenessProbe:
		86  |             httpGet:
		87  |               path: /actuator/health
		88  |               port: 8080
		89  |           readinessProbe:
		90  |             httpGet:
		91  |               path: /actuator/health
		92  |               port: 8080
		93  |           resources:
		94  |             limits:
		95  |               memory: 0.5Gi
		96  |             requests:
		97  |               memory: 0.5Gi
		98  |           env:
		99  |             - name: SPRING_PROFILES_ACTIVE
		100 |               value: default,prod
		101 |             - name: SERVER_PORT
		102 |               value: "80"
		103 |             - name: MANAGEMENT_SERVER_PORT
		104 |               value: "8080"
		105 |             - name: MANAGEMENT_HEALTH_ELASTICSEARCH_ENABLED
		106 |               value: "false"
		107 |             - name: JAVA_TOOL_OPTIONS
		108 |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75
		109 |             - name: SENTRY_DSN
		110 |               value: "https://60337d192ae74aacbe9b16fb6426dbc6@o322024.ingest.sentry.io/5465446"
		111 |             - name: SENTRY_ENVIRONMENT
		112 |               value: 
		113 |             - name: SENTRY_RELEASE
		114 |               value: "0.260725765.0"
		115 |             - name: SENTRY_EXTRA
		116 |               value: "groupId:"


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-policy.default (container 1) - opa
	File: ./charts/policy.yaml:117-146
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		117 |         - name: opa
		118 |           securityContext:
		119 |             {}
		120 |           
		121 |           image: docker.io/openpolicyagent/opa:0.15.1
		122 |           imagePullPolicy: IfNotPresent
		123 |           ports:
		124 |             - name: http
		125 |               containerPort: 8081
		126 |               protocol: TCP
		127 |           livenessProbe:
		128 |             httpGet:
		129 |               path: /health
		130 |               port: 8082
		131 |           readinessProbe:
		132 |             httpGet:
		133 |               path: /health
		134 |               port: 8082
		135 |           args:
		136 |             - "run"
		137 |             - "--server"
		138 |             - "--addr=0.0.0.0:8081"
		139 |             - "--log-level=error"
		140 |             - "--log-format=text"
		141 |             - "--addr=http://0.0.0.0:8082"
		142 |           resources:
		143 |             limits:
		144 |               memory: 0.5Gi
		145 |             requests:
		146 |               memory: 0.5Gi



       _               _              
   ___| |__   ___  ___| | _______   __
  / __| '_ \ / _ \/ __| |/ / _ \ \ / /
 | (__| | | |  __/ (__|   < (_) \ V / 
  \___|_| |_|\___|\___|_|\_\___/ \_/  
                                      
By bridgecrew.io | version: 1.0.846 

kubernetes scan results:

Passed checks: 28, Failed checks: 36, Skipped checks: 0

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_27: "Do not expose the docker daemon socket to containers"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_26

Check: CKV_K8S_19: "Containers should not share the host network namespace"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_18

Check: CKV_K8S_18: "Containers should not share the host IPC namespace"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_17

Check: CKV_K8S_17: "Containers should not share the host process ID namespace"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_16

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

Check: CKV_K8S_13: "Memory limits should be set"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

Check: CKV_K8S_12: "Memory requests should be set"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_25: "Minimize the admission of containers with added capability"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_24

Check: CKV_K8S_39: "Do not use the CAP_SYS_ADMIN linux capability"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_36

Check: CKV_K8S_26: "Do not specify hostPort unless absolutely necessary"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_25

Check: CKV_K8S_15: "Image Pull Policy should be Always"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

Check: CKV_K8S_33: "Ensure the Kubernetes dashboard is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_31

Check: CKV_K8S_16: "Container should not be privileged"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_15

Check: CKV_K8S_35: "Prefer using secrets as files over secrets as environment variables"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_33

Check: CKV_K8S_34: "Ensure that Tiller (Helm v2) is not deployed"
	PASSED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_32

Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Service.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:3-22
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		3  | apiVersion: v1
		4  | kind: Service
		5  | metadata:
		6  |   name: RELEASE-NAME-s3-scan-service
		7  |   labels:
		8  |     app.kubernetes.io/name: s3-scan-service
		9  |     helm.sh/chart: s3-scan-service-0.270944809.0
		10 |     app.kubernetes.io/instance: RELEASE-NAME
		11 |     app.kubernetes.io/version: "0.0.0"
		12 |     app.kubernetes.io/managed-by: Helm
		13 | spec:
		14 |   type: ClusterIP
		15 |   ports:
		16 |     - port: 80
		17 |       targetPort: 80
		18 |       protocol: TCP
		19 |       name: http
		20 |   selector:
		21 |     app.kubernetes.io/name: s3-scan-service
		22 |     app.kubernetes.io/instance: RELEASE-NAME


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-s3-scan-service
		29  |   labels:
		30  |     app.kubernetes.io/name: s3-scan-service
		31  |     helm.sh/chart: s3-scan-service-0.270944809.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: s3-scan-service
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: s3-scan-service
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/s3-scan-service.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/s3-scan-service.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/s3-scan-service.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "s3-scan-service",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--s3-scan-service"
		60  |     spec:
		61  |       containers:
		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-s3-scan-service
		29  |   labels:
		30  |     app.kubernetes.io/name: s3-scan-service
		31  |     helm.sh/chart: s3-scan-service-0.270944809.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: s3-scan-service
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: s3-scan-service
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/s3-scan-service.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/s3-scan-service.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/s3-scan-service.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "s3-scan-service",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--s3-scan-service"
		60  |     spec:
		61  |       containers:
		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-s3-scan-service
		29  |   labels:
		30  |     app.kubernetes.io/name: s3-scan-service
		31  |     helm.sh/chart: s3-scan-service-0.270944809.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: s3-scan-service
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: s3-scan-service
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/s3-scan-service.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/s3-scan-service.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/s3-scan-service.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "s3-scan-service",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--s3-scan-service"
		60  |     spec:
		61  |       containers:
		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-s3-scan-service
		29  |   labels:
		30  |     app.kubernetes.io/name: s3-scan-service
		31  |     helm.sh/chart: s3-scan-service-0.270944809.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: s3-scan-service
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: s3-scan-service
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/s3-scan-service.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/s3-scan-service.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/s3-scan-service.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "s3-scan-service",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--s3-scan-service"
		60  |     spec:
		61  |       containers:
		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-s3-scan-service
		29  |   labels:
		30  |     app.kubernetes.io/name: s3-scan-service
		31  |     helm.sh/chart: s3-scan-service-0.270944809.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: s3-scan-service
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: s3-scan-service
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/s3-scan-service.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/s3-scan-service.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/s3-scan-service.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "s3-scan-service",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--s3-scan-service"
		60  |     spec:
		61  |       containers:
		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:25-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		25  | apiVersion: apps/v1
		26  | kind: Deployment
		27  | metadata:
		28  |   name: RELEASE-NAME-s3-scan-service
		29  |   labels:
		30  |     app.kubernetes.io/name: s3-scan-service
		31  |     helm.sh/chart: s3-scan-service-0.270944809.0
		32  |     app.kubernetes.io/instance: RELEASE-NAME
		33  |     app.kubernetes.io/version: "0.0.0"
		34  |     app.kubernetes.io/managed-by: Helm
		35  | spec:
		36  |   replicas: 1
		37  |   selector:
		38  |     matchLabels:
		39  |       app.kubernetes.io/name: s3-scan-service
		40  |       app.kubernetes.io/instance: RELEASE-NAME
		41  |   template:
		42  |     metadata:
		43  |       labels:
		44  |         app.kubernetes.io/name: s3-scan-service
		45  |         app.kubernetes.io/instance: RELEASE-NAME
		46  |       annotations:
		47  |         ad.datadoghq.com/s3-scan-service.check_names: |
		48  |           ["openmetrics"]
		49  |         ad.datadoghq.com/s3-scan-service.init_configs: |
		50  |           [{}]
		51  |         ad.datadoghq.com/s3-scan-service.instances: |
		52  |           [
		53  |             {
		54  |               "prometheus_url": "http://%%host%%:8080/actuator/prometheus",
		55  |               "namespace": "s3-scan-service",
		56  |               "metrics": ["*"]
		57  |             }
		58  |           ]
		59  |         iam.amazonaws.com/role: "orvn--s3-scan-service"
		60  |     spec:
		61  |       containers:
		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Ingress.RELEASE-NAME-s3-scan-service.default
	File: ./charts/s3-scan-service.yaml:106-137
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		106 | apiVersion: networking.k8s.io/v1beta1
		107 | kind: Ingress
		108 | metadata:
		109 |   name: RELEASE-NAME-s3-scan-service
		110 |   labels:
		111 |     app.kubernetes.io/name: s3-scan-service
		112 |     helm.sh/chart: s3-scan-service-0.270944809.0
		113 |     app.kubernetes.io/instance: RELEASE-NAME
		114 |     app.kubernetes.io/version: "0.0.0"
		115 |     app.kubernetes.io/managed-by: Helm
		116 |   annotations:
		117 |     nginx.ingress.kubernetes.io/auth-response-headers: x-auth-request-user, x-auth-request-email, x-auth-request-access-token
		118 |     nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
		119 |     nginx.ingress.kubernetes.io/configuration-snippet: |
		120 |       proxy_set_header cookie "";
		121 |       auth_request_set $auth_cookie $upstream_http_set_cookie;
		122 |       add_header Set-Cookie $auth_cookie;
		123 |       auth_request_set $bearer_token $upstream_http_x_auth_request_access_token;
		124 |       proxy_set_header authorization 'Bearer $bearer_token';
		125 | spec:
		126 |   rules:
		127 |     - host: 
		128 |       http:
		129 |         paths:
		130 |           - path: /api/data-identifiers
		131 |             backend:
		132 |               serviceName: RELEASE-NAME-s3-scan-service
		133 |               servicePort: 80
		134 |           - path: /api/s3scan
		135 |             backend:
		136 |               serviceName: RELEASE-NAME-s3-scan-service
		137 |               servicePort: 80


Check: CKV_K8S_21: "The default namespace should not be used"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_20

		140 | apiVersion: v1
		141 | kind: Pod
		142 | metadata:
		143 |   name: "RELEASE-NAME-s3-scan-service-test-connection"
		144 |   labels:
		145 |     app.kubernetes.io/name: s3-scan-service
		146 |     helm.sh/chart: s3-scan-service-0.270944809.0
		147 |     app.kubernetes.io/instance: RELEASE-NAME
		148 |     app.kubernetes.io/version: "0.0.0"
		149 |     app.kubernetes.io/managed-by: Helm
		150 |   annotations:
		151 |     "helm.sh/hook": test-success
		152 | spec:
		153 |   containers:
		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']
		158 |   restartPolicy: Never


Check: CKV_K8S_29: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		140 | apiVersion: v1
		141 | kind: Pod
		142 | metadata:
		143 |   name: "RELEASE-NAME-s3-scan-service-test-connection"
		144 |   labels:
		145 |     app.kubernetes.io/name: s3-scan-service
		146 |     helm.sh/chart: s3-scan-service-0.270944809.0
		147 |     app.kubernetes.io/instance: RELEASE-NAME
		148 |     app.kubernetes.io/version: "0.0.0"
		149 |     app.kubernetes.io/managed-by: Helm
		150 |   annotations:
		151 |     "helm.sh/hook": test-success
		152 | spec:
		153 |   containers:
		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']
		158 |   restartPolicy: Never


Check: CKV_K8S_23: "Minimize the admission of root containers"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_22

		140 | apiVersion: v1
		141 | kind: Pod
		142 | metadata:
		143 |   name: "RELEASE-NAME-s3-scan-service-test-connection"
		144 |   labels:
		145 |     app.kubernetes.io/name: s3-scan-service
		146 |     helm.sh/chart: s3-scan-service-0.270944809.0
		147 |     app.kubernetes.io/instance: RELEASE-NAME
		148 |     app.kubernetes.io/version: "0.0.0"
		149 |     app.kubernetes.io/managed-by: Helm
		150 |   annotations:
		151 |     "helm.sh/hook": test-success
		152 | spec:
		153 |   containers:
		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']
		158 |   restartPolicy: Never


Check: CKV_K8S_40: "Containers should run as a high UID to avoid host conflict"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_37

		140 | apiVersion: v1
		141 | kind: Pod
		142 | metadata:
		143 |   name: "RELEASE-NAME-s3-scan-service-test-connection"
		144 |   labels:
		145 |     app.kubernetes.io/name: s3-scan-service
		146 |     helm.sh/chart: s3-scan-service-0.270944809.0
		147 |     app.kubernetes.io/instance: RELEASE-NAME
		148 |     app.kubernetes.io/version: "0.0.0"
		149 |     app.kubernetes.io/managed-by: Helm
		150 |   annotations:
		151 |     "helm.sh/hook": test-success
		152 | spec:
		153 |   containers:
		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']
		158 |   restartPolicy: Never


Check: CKV_K8S_31: "Ensure that the seccomp profile is set to docker/default or runtime/default"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_29

		140 | apiVersion: v1
		141 | kind: Pod
		142 | metadata:
		143 |   name: "RELEASE-NAME-s3-scan-service-test-connection"
		144 |   labels:
		145 |     app.kubernetes.io/name: s3-scan-service
		146 |     helm.sh/chart: s3-scan-service-0.270944809.0
		147 |     app.kubernetes.io/instance: RELEASE-NAME
		148 |     app.kubernetes.io/version: "0.0.0"
		149 |     app.kubernetes.io/managed-by: Helm
		150 |   annotations:
		151 |     "helm.sh/hook": test-success
		152 | spec:
		153 |   containers:
		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']
		158 |   restartPolicy: Never


Check: CKV_K8S_38: "Ensure that Service Account Tokens are only mounted where necessary"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default
	File: ./charts/s3-scan-service.yaml:140-158
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_35

		140 | apiVersion: v1
		141 | kind: Pod
		142 | metadata:
		143 |   name: "RELEASE-NAME-s3-scan-service-test-connection"
		144 |   labels:
		145 |     app.kubernetes.io/name: s3-scan-service
		146 |     helm.sh/chart: s3-scan-service-0.270944809.0
		147 |     app.kubernetes.io/instance: RELEASE-NAME
		148 |     app.kubernetes.io/version: "0.0.0"
		149 |     app.kubernetes.io/managed-by: Helm
		150 |   annotations:
		151 |     "helm.sh/hook": test-success
		152 | spec:
		153 |   containers:
		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']
		158 |   restartPolicy: Never


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_15: "Image Pull Policy should be Always"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_14

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Deployment.RELEASE-NAME-s3-scan-service.default (container 0) - s3-scan-service
	File: ./charts/s3-scan-service.yaml:62-103
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		62  |         - name: s3-scan-service
		63  |           
		64  |           image: "registry.gitlab.com/openraven/open/s3-scan-service-repo-docker:270944809"
		65  |           imagePullPolicy: IfNotPresent
		66  |           ports:
		67  |             - name: http
		68  |               containerPort: 80
		69  |               protocol: TCP
		70  |           livenessProbe:
		71  |             httpGet:
		72  |               path: /actuator/health
		73  |               port: 8080
		74  |           readinessProbe:
		75  |             httpGet:
		76  |               path: /actuator/health
		77  |               port: 8080
		78  |           env:
		79  |             - name: SPRING_PROFILES_ACTIVE
		80  |               value: default,prod
		81  |             - name: MANAGEMENT_SERVER_PORT
		82  |               value: '8080'
		83  |             - name: JAVA_TOOL_OPTIONS
		84  |               value: -XX:+UseContainerSupport -XX:MaxRAMPercentage=70
		85  |             - name: SENTRY_DSN
		86  |               value: "https://eea6b3c7bbcb4b9cbbdfccef714d42cd@o322024.ingest.sentry.io/5406671"
		87  |             - name: SENTRY_ENVIRONMENT
		88  |               value: 
		89  |             - name: SENTRY_RELEASE
		90  |               value: "0.270944809.0"
		91  |             - name: SENTRY_EXTRA
		92  |               value: "groupId:"
		93  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-ID
		94  |               value: 
		95  |             - name: SPRING_SECURITY_OAUTH2_CLIENT_REGISTRATION_ASGARD_CLIENT-SECRET
		96  |               value: 
		97  |             - name: OPENRAVEN_APP_V1_CLUSTER_NAME
		98  |               value: 
		99  |           resources:
		100 |             limits:
		101 |               memory: 1Gi
		102 |             requests:
		103 |               memory: 1Gi


Check: CKV_K8S_20: "Containers should not run with allowPrivilegeEscalation"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_19

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_30: "Apply security context to your pods and containers"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_28

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_11: "CPU limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_10

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_10: "CPU requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_9

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_28: "Minimize the admission of containers with the NET_RAW capability"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_27

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_43: "Image should use digest"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_39

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_14: "Image Tag should be fixed - not latest or blank"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_13

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_8: "Liveness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_7

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_13: "Memory limits should be set"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_12

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_12: "Memory requests should be set"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_11

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_37: "Minimize the admission of containers with capabilities assigned"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_34

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_9: "Readiness Probe Should be Configured"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_8

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


Check: CKV_K8S_22: "Use read-only filesystem for containers where possible"
	FAILED for resource: Pod.RELEASE-NAME-s3-scan-service-test-connection.default (container 0) - wget
	File: ./charts/s3-scan-service.yaml:154-157
	Guide: https://docs.bridgecrew.io/docs/bc_k8s_21

		154 |     - name: wget
		155 |       image: busybox
		156 |       command: ['wget']
		157 |       args:  ['RELEASE-NAME-s3-scan-service:80']


